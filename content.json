{"pages":[{"title":"关于我","text":"非典型码农，喜好科技资讯，爱装机。航拍，全景制图萌新。 中土世界。 已点属性： 三流段子手，机评二十年。 C++/Java程序猿。 Less is More. 泛学ing 待续 嗯 ps： 没事的时候多看书准没错，学习新技术，多运动锻炼。","link":"/about/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"C++中sprintf函数的用法","text":"1.常用方式 sprintf函数的功能与printf函数的功能基本一样，只是它把结果输出到指定的字符串中了，看个例子就明白了： 例：将&quot;test,1,2&quot;写入数组s中 12345678910#include&lt;stdio.h&gt;int main(int argc, char *avgv[]){ char s[40]; sprintf(s,&quot;%s%d%c&quot;,&quot;test&quot;,1,'2'); /*第一个参数就是指向要写入的那个字符串的指针，剩下的就和printf()一样了 你可以比较一下，这是向屏幕输入*/ printf(&quot;%s%d%c&quot;,&quot;test&quot;,1,'2'); return 0;} 编译： g++ sprinftest.cpp -o sprinftest &amp;&amp; ./sprinftest 输出结果： sprintftest12sprintftest12 2.若”%s”等输出符在字符串中例：补全字符串str的缺省内容 123456789101112#include &lt;iostream&gt;#include &lt;stdio.h&gt;#include &lt;cstring&gt;int main(int argc, char *avgv[]){ char str[] = &quot;hel%co wo%sd! sp%stf test%d&quot;; char buf[strlen(str)]; sprintf(buf, str, 'l', &quot;rl&quot;, &quot;rin&quot;, 1); std::cout &lt;&lt; &quot;str = &quot;&lt;&lt; buf &lt;&lt; &quot;\\nlen = &quot; &lt;&lt; strlen(buf) &lt;&lt; std::endl; return 0;} 编译： g++ sprinftest.cpp -o sprinftest &amp;&amp; ./sprinftest 输出结果： str = hello world! sprintf test1len = 27 这种形式也可以将多个字符值或字符串值赋值到字符串str中，有多少个输出符就后面就加多少个参数。","link":"/2015/07/21/C++%E4%B8%ADsprintf%E5%87%BD%E6%95%B0%E7%9A%84%E7%94%A8%E6%B3%95/"},{"title":"Elasticsearch实践(1)-搭建及IK中文分词","text":"🔍近期在做商品搜索优化，对比了Solr与Elasticsearch的区别，两者都是基于Lucene实现的封装，但es在数据量越大的情况下实时检索性能优于solr，更适用于实时商品搜索，于是选用了es，下面介绍elasticsearch搜索引擎的搭建及使用，这里使用的是2.2.1的es版本，以及1.8.1的ik分词版本。 安装es及IK中文分词1. 下载安装包12345##我的环境是Redhat6.4，此处使用的是es2.2.1版本##（1）可以使用指令安装$ yum install https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/rpm/elasticsearch/2.2.1/elasticsearch-2.2.1.rpm##（2）也可以直接下载文件，然后上传至linux目录下https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.2.1/elasticsearch-2.2.1.tar.gz 2. 上传安装文件到linux目录并解压我这里将elasticsearch-2.2.1.tar.gz文件放至/opt/elasticsearch目录下。 解压： 12$ cd /opt/elasticsearch$ tar -zxvf elasticsearch-2.2.1.tar.gz 此时切记不能用 ./bin/elasticsearch启动，因为在root权限下会提示错误： 1Exception in thread &quot;main&quot; java.lang.RuntimeException: don't run elasticsearch as root. 3. 新添加一个elasticsearch用户123456789101112#添加elasticsearch用户：$ useradd elasticsearch#给用户elasticsearch设置密码，连续输入2次：$ passwd elasticsearch#创建一个用户组es:$ groupadd es#分配elasticsearch到es组:$ usermod -G es elasticsearch#在elasticsearch 根目录下，给定用户权限。-R表示逐级（N层目录） ， * 表示 任何文件:$ chown -R elasticsearch:es *#切换到elasticsearch用户:$ su elasticsearch 4. 修改配置文件1$ vi config/elasticsearch.yml 修改以下内容： 123456789#cluster namecluster.name: my-application#节点名称node.name: node-1path.data: /opt/elasticsearch/elasticsearch-2.2.1/datapath.logs: /opt/elasticsearch/elasticsearch-2.2.1/logs#绑定IP和端口network.host: 0.0.0.0http.port: 9200 5. 安装headhead是elasticsearch的可视化工具，可以不用安装head，如果不需要安装，跳过此步骤。 需要注意的是，es2.x的版本head可直接安装，es5.x以上的版本需要先安装nodejs。 5.1. ES2.x.x版本安装head在/opt/elasticsearch/elasticsearch-2.2.1/目录下新建文件夹：plugins 1234$ cd /opt/elasticsearch/elasticsearch-2.2.1/$ mkdir plugins$ cd elasticsearch/bin$ ./plugin install mobz/elasticsearch-head 安装成功即可。启动es之后，访问http://ip:9200/_plugin/head/即可看到可视化界面。 5.2 ES5.x.x版本安装headES5.x版本暂时不支持直接安装，但是head作者提供了另一种安装方法。 1234$ git clone git://github.com/mobz/elasticsearch-head.git$ cd elasticsearch-head$ npm install$ npm run start 然后打开http://localhost:9100/访问即可。 如果提示未安装node.js，先安装node.js， node.js官网：http://nodejs.cn/download/ 选择安装包，我这边是64位linux，下载的是https://nodejs.org/dist/v8.11.3/node-v8.11.3-linux-x64.tar.xz 解压（注：如果是虚拟机，不要在共享文件夹中解压，会报Cannot create symlink to创建软链的错，请放至linux下解压）： 12345678$ xz -d node-v8.11.3-linux-x64.tar.xz$ tar -xvf node-v8.11.3-linux-x64.tar$ mv node-v8.11.3-linux-x64 nodejs#确认一下nodejs下bin目录是否有node 和npm文件，如果有执行软连接，如果没有重新下载执行上边步骤；建立软连接，变为全局:$ ln -s /opt/nodejs/bin/npm /usr/local/bin/$ ln -s /opt/nodejs/bin/node /usr/local/bin/#检查是否安装成功$ node -v 6. 启动ES123$ cd /opt/elasticsearch/elasticsearch-2.2.1$ su elasticsearch$ ./bin/elasticsearch 再访问http://ip:9200/_plugin/head/即可。 7. 安装IK分词器在https://github.com/medcl/elasticsearch-analysis-ik 此处选择与es版本对应的ik分词版本，我这里下载的是elasticsearch-analysis-ik-1.8.1.zip，在 /opt/elasticsearch/elasticsearch-2.2.1/plugins目录下新建文件夹：ik;将elasticsearch-analysis-ik-1.8.1.zip的解压文件放入ik目录下，并将解压后的config下的ik文件夹放入 /opt/elasticsearch/elasticsearch-2.2.1/config下面; 123456$ unzip elasticsearch-analysis-ik-1.8.1.zip$ cd /opt/elasticsearch/elasticsearch-2.2.1/plugins$ mkdir ik$ cp -rf /opt/elasticsearch/elasticsearch-analysis-ik-1.8.1/ ik/$ mv /opt/elasticsearch/elasticsearch-analysis-ik-1.8.1/config/ik /opt/elasticsearch/elasticsearch-2.2.1/config$ cd /opt/elasticsearch/elasticsearch-2.2.1/config 在/config/elasticsearch.yml文件的最后加上一句话： 1index.analysis.analyzer.ik.type : &quot;ik&quot; 然后启动es，测试分词效果： 1234curl '10.10.13.234:9200/index/_analyze?analyzer=ik&amp;pretty=true' -d '{&quot;text&quot;:&quot;世界如此之大 进口水果&quot;}' 或者浏览器访问http://ip:9200/_analyze?analyzer=ik&amp;pretty=true&amp;text=sojson在线工具测试分词，即可看到中文分词效果。 至此，elasticsearch搭建完成，下一篇我们来说下索引的创建。","link":"/2018/05/15/Elasticsearch%E5%AE%9E%E8%B7%B5%E6%90%AD%E5%BB%BA%E5%8F%8AIK%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"},{"title":"Elasticsearch实践(2)-索引及索引别名alias","text":"我们都知道es效率如此高主要和索引是分不开的，需要将每一条数据建立索引，创建索引时数据字段也是你插入时的样子，索引中包含了数据的属性字段。而且创建索引也比较耗时（当然了，肯定比插入关系型数据库中更快😁），但是毕竟每一条数据都要建一次，数据量到达千万级亿级时，时间不是很乐观。🤔 💡想像这样一个场景，产品上线一段时间后，由于产品需要，要将某个字段类型改变，比如需要将某个long型字段改成string类型，直接改会类型报错，查阅官方文档可知，es是不支持索引的更新操作的，需要对所有已有数据的所有field进行reindex，这意味着，需要停止服务进行重建索引操作，停服是最不想看到😭，这时如果你当初建了索引别名，你会感谢你当初使用别名的决定，为何要用别名alias？🤷 Elasticsearch的索引别名alias，类似于数据库的视图。因为索引别名alias是映射到之前已创建的索引上的，对你要创建新的索引毫无影响。在创建好新索引之后，我们只需将这别名alias映射到新创建的别名上即可，这些操作只需要在服务器上通过指令操作即可，完全不影响线上服务，实现无缝切换索引。然后再将老的索引删掉即可，以节省空间。 1. 创建一个索引创建一个索引，名为goods： 1$ curl -XPUT 10.10.13.234:9200/goods 2. 为索引创建mapping为索引goods创建mapping，这里制定content字段属性使用ik中文分词器，只要制定搜索content内容就会查询出分词之后的关联结果： 1234567891011121314151617$ curl -XPOST 10.10.13.234:9200/goods/fulltext/_mapping -d'{ &quot;fulltext&quot;: { &quot;_all&quot;: { &quot;analyzer&quot;: &quot;ik&quot; }, &quot;properties&quot;: { &quot;content&quot;: { &quot;type&quot; : &quot;string&quot;, &quot;boost&quot; : 8.0, &quot;term_vector&quot; : &quot;with_positions_offsets&quot;, &quot;analyzer&quot; : &quot;ik&quot;, &quot;include_in_all&quot; : true } } }}' 这里创建的索引goods，如果未使用别名，程序中也是用goods这个索引名，如果建了新的索引，那么程序也要用新的索引名称，这是不友好的，如果用了索引别名alias就可以解决这个问题，继续看后面的操作。 3. 插入数据插入一些测试数据： 1234567891011121314$ curl -XPOST 10.10.13.234:9200/goods/fulltext/1 -H 'Content-Type:application/json' -d'{ &quot;content&quot;: &quot;进口华盛顿红蛇果 苹果4个装 单果重约180g 新鲜水果&quot; }'$ curl -XPOST 10.10.13.234:9200/goods/fulltext/2 -H 'Content-Type:application/json' -d'{ &quot;content&quot;: &quot;华盛顿苹果礼盒 8个装（4个红蛇果+4个青苹果）单果重约145g-180g 新鲜水果礼盒&quot; }'$ curl -XPOST 10.10.13.234:9200/goods/fulltext/4 -H 'Content-Type:application/json' -d'{ &quot;content&quot;: &quot;新疆阿克苏冰糖心 约5kg 单果200-250g（7Fresh 专供）&quot; }'$ curl -XPOST 10.10.13.234:9200/goods/fulltext/5 -H 'Content-Type:application/json' -d'{ &quot;content&quot;: &quot;果花 珍珠岩 无土栽培基质 颗粒状 保温性能好 园艺用品&quot; }'#查看goods的mpping，后面需要与索引别名的mapping对比$ curl -XGET &quot;10.10.13.234:9200/goods/_mapping?pretty&quot; 4. 测试分词高亮测试命中分词高亮： 12345678910$ curl -XPOST 10.10.13.234:9200/goods/fulltext/_search -H 'Content-Type:application/json' -d'{ &quot;query&quot; : { &quot;match&quot; : { &quot;content&quot; : &quot;进口水果&quot; }}, &quot;highlight&quot; : { &quot;pre_tags&quot; : [&quot;&lt;tag1&gt;&quot;, &quot;&lt;tag2&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/tag1&gt;&quot;, &quot;&lt;/tag2&gt;&quot;], &quot;fields&quot; : { &quot;content&quot; : {} } }}' 5. 创建索引别名新建同义词mapping： (1) 创建一个索引，这个索引的名称最好带上版本号，比如my_index_v1,my_index_v2等; (2) 创建一个指向本索引的同义词。为的是以后不停服更新mapping。 步骤(1)的索引我们以刚刚创建的goods所以为例，创建别名映射到goods所以上： 123456789$ curl -XPOST 10.10.13.234:9200/_aliases -d ' { &quot;actions&quot;: [ { &quot;add&quot;: { &quot;alias&quot;: &quot;my_index&quot;, &quot;index&quot;: &quot;goods&quot; }} ] }' 然后查看索引my_index的mapping： 1$ curl -XGET &quot;10.10.13.234:9200/my_index/_mapping?pretty&quot; 索引my_index与goods的mapping一致。 6. 切换索引再创建一个索引goods_v2，重复步骤1、2，名称换成goods_v2 123456789101112131415161718$ curl -XPUT 10.10.13.234:9200/goods_v2$ curl -XPOST 10.10.13.234:9200/goods_v2/fulltext/_mapping -d'{ &quot;fulltext&quot;: { &quot;_all&quot;: { &quot;analyzer&quot;: &quot;ik&quot; }, &quot;properties&quot;: { &quot;content&quot;: { &quot;type&quot; : &quot;string&quot;, &quot;boost&quot; : 8.0, &quot;term_vector&quot; : &quot;with_positions_offsets&quot;, &quot;analyzer&quot; : &quot;ik&quot;, &quot;include_in_all&quot; : true } } }}' 插入测试数： 123456$ curl -XPOST 10.10.13.234:9200/goods_v2/fulltext/1 -H 'Content-Type:application/json' -d'{ &quot;content&quot;: &quot;进口华盛顿红蛇果 苹果4个装 单果重约180g 新鲜水果&quot;,&quot;number&quot;: 21}'#查看goods_v2的mpping，后面需要与索引别名的mapping对比$ curl -XGET &quot;10.10.13.234:9200/goods_v2/_mapping?pretty&quot; 重点是更换索引的操作，将my_index别名切换映射到goods_v2上： 12345678910111213141516$ curl -XPOST 10.10.13.234:9200/_aliases -d '{ &quot;actions&quot;: [ { &quot;remove&quot;: { &quot;alias&quot;: &quot;my_index&quot;, &quot;index&quot;: &quot;goods&quot; }}, { &quot;add&quot;: { &quot;alias&quot;: &quot;my_index&quot;, &quot;index&quot;: &quot;goods_v2&quot; }} ]}'#查看my_index的mpping，与前面goods_v2的mapping对比$ curl -XGET &quot;10.10.13.234:9200/my_index/_mapping?pretty&quot; 我们可以对比索引my_index与goods_v2的mapping是一致的，索引切换成功。 创建了索引别名alias的话，程序中制定这个my_index这个别名即可，后需要创建新索引，只需切换索引映射到新索引上即可，实现不停服更新mapping。 下一篇我们来说下通过创建spring boot工程，通过es的java api操作es，实现分页查询和中文分词检索的功能。","link":"/2018/05/16/Elasticsearch%E5%AE%9E%E8%B7%B5%E7%B4%A2%E5%BC%95%E5%8F%8A%E7%B4%A2%E5%BC%95%E5%88%AB%E5%90%8Dalias/"},{"title":"Packet for query is too large (84 &gt; -1).","text":"ibatis工程使用resin配置mysql出现Packet for query is too large (84 &gt; -1).错误。 windows下的resin配置连接mysql，常用的安全的做法是将数据库信息配置到conf目录下的resin.xml文件中。 因为resin连接mysql不是必须的，所以resin本身没有提供mysql-connector的jar包，需要自己加到resin目录下的lib里面，我加了个mysql-connector-java-5.1.45-bin.jar进去，然而在运行工程执行数据库查询操作的时候，却出现了问题，报如下错误： 123456789org.springframework.dao.TransientDataAccessResourceException: SqlMapClient operation; SQL []; --- The error occurred while applying a parameter map. --- Check the T_USER_INFO.selectByMap-InlineParameterMap. --- Check the statement (query failed). --- Cause: com.mysql.jdbc.PacketTooBigException: Packet for query is too large (84 &gt; -1). You can change this value on the server by setting the max_allowed_packet' variable.; nested exception is com.ibatis.common.jdbc.exception.NestedSQLException: --- The error occurred while applying a parameter map. --- Check the T_USER_INFO.selectByMap-InlineParameterMap. --- Check the statement (query failed). --- Cause: com.mysql.jdbc.PacketTooBigException: Packet for query is too large (84 &gt; -1). You can change this value on the server by setting the max_allowed_packet' variable. 只进行查询却出现Packet for query is too large，而且查询的条件只有几个字母，网上搜了很多解答，都是说需要把mysql的max_allowed_packet参数调大，但是我通过show VARIABLES like '%max_allowed_packet%';查询的结果也很大，如果是参数过小，应该是提示大于数据库中设置的packet的值，而不应该报-1。 最后发现有个帖子说是mysql-connector-javajar包的版本问题，版本不能太高，随后我将mysql-connector-java-5.1.45-bin.jar换成mysql-connector-java-5.1.26-bin.jar，不再出现packet错误，问题解决。 可能是因为我用的是ibatis2.0版本的缘故，不支持太高的mysql连接jar版本，换成mysql-connector-java-5.1.40以下版本即可解决Packet for query is too large (84 &gt; -1)问题。","link":"/2018/03/15/Packet%20for%20query%20is%20too%20large/"},{"title":"Socket send函数和recv函数详解","text":"1. send 函数**int send( SOCKET s, const char FAR *buf, int len, int flags ); ** 不论是客户还是服务器应用程序都用send函数来向TCP连接的另一端发送数据。客户程序一般用send函数向服务器发送请求，而服务器则通常用send函数来向客户程序发送应答。 ​ 该函数的第一个参数指定发送端套接字描述符； ​ 第二个参数指明一个存放应用程序要发送数据的缓冲区； ​ 第三个参数指明实际要发送的数据的字节数； ​ 第四个参数一般置0。 ​ 这里只描述同步Socket的send函数的执行流程。当调用该函数时， （1）send先比较待发送数据的长度len和套接字s的发送缓冲的长度， 如果len大于s的发送缓冲区的长度，该函数返回SOCKET_ERROR； （2）如果len小于或者等于s的发送缓冲区的长度，那么send先检查协议是否正在发送s的发送缓冲中的数据，如果是就等待协议把数据发送完，如果协议还没有开始发送s的发送缓冲中的数据或者s的发送缓冲中没有数据，那么send就比较s的发送缓冲区的剩余空间和len （3）如果len大于剩余空间大小，send就一直等待协议把s的发送缓冲中的数据发送完 （4）如果len小于剩余 空间大小，send就仅仅把buf中的数据copy到剩余空间里（注意并不是send把s的发送缓冲中的数据传到连接的另一端的，而是协议传的，send仅仅是把buf中的数据copy到s的发送缓冲区的剩余空间里）。 ​ 如果send函数copy数据成功，就返回实际copy的字节数，如果send在copy数据时出现错误，那么send就返回SOCKET_ERROR；如果send在等待协议传送数据时网络断开的话，那么send函数也返回SOCKET_ERROR。 ​ 要注意send函数把buf中的数据成功copy到s的发送缓冲的剩余空间里后它就返回了，但是此时这些数据并不一定马上被传到连接的另一端。如果协议在后续的传送过程中出现网络错误的话，那么下一个Socket函数就会返回SOCKET_ERROR。（每一个除send外的Socket函数在执 行的最开始总要先等待套接字的发送缓冲中的数据被协议传送完毕才能继续，如果在等待时出现网络错误，那么该Socket函数就返回 SOCKET_ERROR） 注意：在Unix系统下，如果send在等待协议传送数据时网络断开的话，调用send的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。 通过测试发现，异步socket的send函数在网络刚刚断开时还能发送返回相应的字节数，同时使用select检测也是可写的，但是过几秒钟之后，再send就会出错了，返回-1。select也不能检测出可写了。 2. recv函数 int recv( SOCKET s, char FAR *buf, int len, int flags); 不论是客户还是服务器应用程序都用recv函数从TCP连接的另一端接收数据。该函数的第一个参数指定接收端套接字描述符； ​ 第二个参数指明一个缓冲区，该缓冲区用来存放recv函数接收到的数据； ​ 第三个参数指明buf的长度； ​ 第四个参数一般置0。 ​ 这里只描述同步Socket的recv函数的执行流程。当应用程序调用recv函数时， ​ （1）recv先等待s的发送缓冲中的数据被协议传送完毕，如果协议在传送s的发送缓冲中的数据时出现网络错误，那么recv函数返回SOCKET_ERROR ； ​ （2）如果s的发送缓冲中没有数据或者数据被协议成功发送完毕后，recv先检查套接字s的接收缓冲区，如果s接收缓冲区中没有数据或者协议正在接收数据，那么recv就一直等待，直到协议把数据接收完毕。当协议把数据接收完毕，recv函数就把s的接收缓冲中的数据copy到buf中（注意协议接收到的数据可能大于buf的长度，所以 在这种情况下要调用几次recv函数才能把s的接收缓冲中的数据copy完。recv函数仅仅是copy数据，真正的接收数据是协议来完成的）； ​ recv函数返回其实际copy的字节数。如果recv在copy时出错，那么它返回SOCKET_ERROR；如果recv函数在等待协议接收数据时网络中断了，那么它返回0。 注意：在Unix系统下，如果recv函数在等待协议接收数据时网络断开了，那么调用recv的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。","link":"/2017/01/10/Socket_send%E5%87%BD%E6%95%B0%E5%92%8Crecv%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/"},{"title":"TCP长连接与短连接的区别","text":"1. TCP连接​ 当网络通信时采用TCP协议时，在真正的读写操作之前，server与client之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时它们可以释放这个连接，连接的建立是需要三次握手的，而释放则需要4次握手，所以说每个连接的建立都是需要资源消耗和时间消耗的。 经典的三次握手示意图： 经典的四次握手关闭图： 2. TCP短连接​ 我们模拟一下TCP短连接的情况，client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起close操作。 为什么呢，一般的server不会回复完client后立即关闭连接的，当然不排除有特殊的情况。 从上面的描述看，短连接一般只会在client/server间传递一次读写操作。 短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。 3. TCP长连接​ 接下来我们再模拟一下长连接的情况，client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 ​ 首先说一下TCP/IP详解上讲到的TCP保活功能，保活功能主要为服务器应用提供，服务器应用希望知道客户主机是否崩溃，从而可以代表客户使用资源。如果客户已经消失，使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，则服务器将应远等待客户端的数据，保活功能就是试图在服务器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一： 客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。 ​ 从上面可以看出，TCP保活功能主要为探测长连接的存活状况，不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。 在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略， 如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致server端服务受损； 如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服。 长连接和短连接的产生在于client和server采取的关闭策略，具体的应用场景采用具体的策略，没有十全十美的选择，只有合适的选择。","link":"/2015/02/20/TCP%E9%95%BF%E8%BF%9E%E6%8E%A5%E4%B8%8E%E7%9F%AD%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"linux-crontab定时执行任务","text":"⏱️最近碰到一个关于crontab的问题。 1. 事因服务器部署了一个C++查询数据库词库的服务，以及Java发送目标词的服务，Java服务通过soket长连接向C++服务发送目标单词，然后C++服务返回数据库中是否存在的结果。 期间，由于数据库有时增加单词需要重启服务，手写了个定时脚本来重启该C++服务。 某一次为了调试，把改定时脚本关了，通过ps -ef |grep xxx命令查看服务父子进程情况，发现父进程还是每隔十分钟退出，服务会重启，想了许久不知道哪出了问题。 最后询问运维才想起，之前还用了linux自带的crontab设置过每隔十分钟重启服务，没有删除crontab里面的那条设置。 所以我们就来学习一下linux自带的可设置定时指定任务的crontab。 2. 初识crontabcron是一个服务进程，cron服务提供crontab命令来设定cron服务的，以下是这个命令的一些参数与说明： crontab -u //设定某个用户的cron服务，一般root用户在执行这个命令的时候需要此参数 crontab -l //列出某个用户cron服务的详细内容 crontab -r //删除某个用户的cron服务 crontab -e //编辑某个用户的cron服务 3. 基本用法基本格式 : 1* * * * * command ​ **分 时 日 月 周 命令 ** 第1列表示分钟1～59 每分钟用*或者 */1表示 ；第2列表示小时1～23（0表示0点） ；第3列表示日期1～31 ；第4列表示月份1～12 ；第5列标识号星期0～6（0表示星期天） ；第6列要运行的命令 。 crontab的一些使用例子: */10 * * * * (cd /opt/resin/bin; ./resin.sh restart) 表示每隔10分重启resin服务； 30 1 * * * (cd /opt/resin/bin; ./resin.sh restart) 表示每天1点30分重启resin服务； * 23-5/1 * * * (cd /opt/resin/bin; ./resin.sh restart) 表示每天23点到次日5点之间每隔1小时重启resin服务；","link":"/2017/07/25/linux-crontab%E5%AE%9A%E6%97%B6%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1/"},{"title":"mongodb学习笔记--$elemMatch操作符的使用","text":"mongodb提供了很多操作符以便更加方便快捷地操作数据，下面我们来认识一下查询操作符$elemMatch，$elemMatch投影操作符将限制查询返回的数组字段的内容只包含匹配$elemMatch条件的数组元素。 注意： 数组中元素是内嵌文档。 如果多个元素匹配$elemMatch条件，操作符返回数组中第一个匹配条件的元素。 1.首先创建一个简单文档1db.test.insert({&quot;id&quot;:1, &quot;members&quot;:[{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:27, &quot;gender&quot;:&quot;M&quot;}, {&quot;name&quot;:&quot;BuleRiver2&quot;, &quot;age&quot;:23, &quot;gender&quot;:&quot;F&quot;}, {&quot;name&quot;:&quot;BuleRiver3&quot;, &quot;age&quot;:21, &quot;gender&quot;:&quot;M&quot;}]}); 2.使用多种方式尝试查询(1) 使用db.test.find({&quot;members&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;}});进行查询： 1db.test.find({&quot;members&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;}}); 查询的结果是空集。 (2) 只有完全匹配一个的时候才能获取到结果，因此： 1db.test.find({&quot;members&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:27, &quot;gender&quot;:&quot;M&quot;}}); 可以得到结果。 (3) 如果把键值进行颠倒,也得不到结果： 1db.test.find({&quot;members&quot;:{&quot;age&quot;:27, &quot;name&quot;:&quot;BuleRiver1&quot;, &quot;gender&quot;:&quot;M&quot;}}); 得到的结果是空集。 (4) 我们这样查询： 1db.test.find({&quot;members.name&quot;:&quot;BuleRiver1&quot;}); 是可以查询出结果的。 (5) 如果需要两个属性： 1db.test.find({&quot;members.name&quot;:&quot;BuleRiver1&quot;, &quot;members.age&quot;:27}); 也可以查询出结果。 (6) 我们再进行破坏性尝试： 1db.test.find({&quot;members.name&quot;:&quot;BuleRiver1&quot;, &quot;members.age&quot;:23}); 也可以查询出结果。 不过我们应该注意到：BuleRiver1是数组中第一个元素的键值，而23是数组中第二个元素的键值，这样也可以查询出结果。 3.使用$elemMatch操作符查询 对于我们的一些应用来说，以上结果显然不是我们想要的结果。所以我们应该使用$elemMatch操作符: (1)``$elemMatch+同一个元素中的键值组合 1db.test.find({&quot;members&quot;:{&quot;$elemMatch&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:27}}}); 可以查询出结果； (2)$elemMatch+不同元素的键值组合 1db.test.find({&quot;members&quot;:{&quot;$elemMatch&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:23}}}); 查询不出结果。 因此，(1)展示的嵌套查询正是我们想要的查询方式。","link":"/2017/02/21/mongodb%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--$elemMatch%E6%93%8D%E4%BD%9C%E7%AC%A6%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"mongodb学习笔记--C++操作mongodb","text":"在学习mongodb过程当中，必须学习的就是用C++（Java、PHP、C#等）操作mongodb，这里讲述C++操作mongodb，在官方提供的mongo-cxx-driver驱动中有相关的操作例子，可以结合例子学习，目录是mongo-cxx-driver-legacy-1.0.0-rc0\\src\\mongo\\client\\examples，这里针对几个重要的点讲述。 ##1. C++连接mongodb （1）有无密码都适用 123456789101112131415161718192021mongo::client::GlobalInstance instance;if (!instance.initialized()) { std::cout &lt;&lt; &quot;failed to initialize the client driver: &quot; &lt;&lt; instance.status() &lt;&lt; std::endl; return EXIT_FAILURE;}std::string uri = &quot;mongodb://username:password@127.0.0.1:27017&quot;;std::string errmsg;ConnectionString cs = ConnectionString::parse(uri, errmsg);if (!cs.isValid()) { std::cout &lt;&lt; &quot;Error parsing connection string &quot; &lt;&lt; uri &lt;&lt; &quot;: &quot; &lt;&lt; errmsg &lt;&lt; std::endl; return EXIT_FAILURE;}boost::scoped_ptr&lt;DBClientBase&gt; conn(cs.connect(errmsg));if (!conn) { cout &lt;&lt; &quot;couldn't connect : &quot; &lt;&lt; errmsg &lt;&lt; endl; return EXIT_FAILURE;} 这种方式有密码和无密码的情况都适用，&quot;username&quot;为登录名，&quot;password&quot;为登录密码；无密码时去掉&quot;username:password@&quot;即可连接，这样连接方式比较常用。 （2）常规连接操作（不带密码连接） 1234567891011mongo::DBClientConnection conn; mongo::Status status = mongo::client::initialize();if (!status.isOK()) { MongoException m(-1, &quot;failed to initialize the client driver: &quot; + status.toString()); return -1;}string url = &quot;localhost:27017&quot;;if (!conn.connect(url, errmsg)) { MongoException m(0, &quot;couldn’t connect : &quot; + errmsg); return -1;} 通过这种方式可建立与mongodb的连接，但是是未带权限的连接。 （3）带密码连接 1conn.auth( &quot;admin&quot; , &quot;username&quot; , &quot;password&quot; , errmsg ); 其中&quot;admin&quot;为验证用户的所在数据库，一般在&quot;test.system.users&quot;或&quot;admin.system.users&quot;表中，所以填库名&quot;test&quot;或&quot;admin&quot;，&quot;username&quot;为登录名，&quot;password&quot;为登录密码。或者下面的auth方式也可以： 1234conn.auth(BSON( &quot;user&quot; &lt;&lt; &quot;root&quot; &lt;&lt; &quot;db&quot; &lt;&lt; &quot;admin&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;password&quot; &lt;&lt; &quot;mechanism&quot; &lt;&lt; &quot;DEFAULT&quot;)); 根据mongodb的设置更改传递给函数conn.auth的参数值。 ##2. C++查询数据 123const char* ns = &quot;test.first&quot;;conn-&gt;findOne(ns, BSONObj()); //查询一条数据conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); //条件查询 ##3. C++插入数据 1234const char* ns = &quot;test.first&quot;;conn-&gt;insert(ns, BSON(&quot;name&quot; &lt;&lt; &quot;joe&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;123456&quot; &lt;&lt; &quot;age&quot; &lt;&lt; 20)); ##4. C++删除数据 123const char* ns = &quot;test.first&quot;;conn-&gt;remove(ns, BSONObj()); //删除所有conn-&gt;remove(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); //删除指定文档 ##5. C++更新数据 (1) 在原数据基础上增加属性 1234const char* ns = &quot;test.first&quot;;BSONObj res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj());BSONObj after = BSONObjBuilder().appendElements(res).append(&quot;name2&quot;, &quot;h&quot;).obj();conn-&gt;update(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;jeo2&quot;).obj(), after); //在原数据上增加属性 这种update方式只是在原数据基础上增加属性，并不会改变指定属性值。(2) 更改指定的某个属性值**想要update指定属性的值，需要用到”$set”指令**，如下代码所示： 123BSONObj res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj());//更改指定的某个属性值conn-&gt;update(&quot;test.first&quot;, res, BSON(&quot;$set&quot; &lt;&lt; BSON(&quot;age&quot; &lt;&lt; 11))); 主要想强调下update更改指定的某个属性值的方式，比较常用。 ##6. C++操作例子(1) simple_client_test.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127/*#if defined(_WIN32)#include &lt;winsock2.h&gt;#include &lt;windows.h&gt;#endif*///g++ simple_client_test.cpp -pthread -lmongoclient -lboost_thread -lboost_system -lboost_regex -o simple_client_test#include &quot;mongo/client/dbclient.h&quot; // the mongo c++ driver#include &lt;iostream&gt;#ifndef verify#define verify(x) MONGO_verify(x)#endifusing namespace std;using namespace mongo;//using mongo::BSONObj;//using mongo::BSONObjBuilder;int main(int argc, char* argv[]) { if (argc &gt; 2) { std::cout &lt;&lt; &quot;usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; [MONGODB_URI]&quot; &lt;&lt; std::endl; return EXIT_FAILURE; } mongo::client::GlobalInstance instance; if (!instance.initialized()) { std::cout &lt;&lt; &quot;failed to initialize the client driver: &quot; &lt;&lt; instance.status() &lt;&lt; std::endl; return EXIT_FAILURE; } std::string uri = argc == 2 ? argv[1] : &quot;mongodb://127.0.0.1:27017&quot;; std::string errmsg; ConnectionString cs = ConnectionString::parse(uri, errmsg); if (!cs.isValid()) { std::cout &lt;&lt; &quot;Error parsing connection string &quot; &lt;&lt; uri &lt;&lt; &quot;: &quot; &lt;&lt; errmsg &lt;&lt; std::endl; return EXIT_FAILURE; } boost::scoped_ptr&lt;DBClientBase&gt; conn(cs.connect(errmsg)); if (!conn) { cout &lt;&lt; &quot;couldn't connect : &quot; &lt;&lt; errmsg &lt;&lt; endl; return EXIT_FAILURE; } const char* ns = &quot;test.first&quot;; conn-&gt;dropCollection(ns); //clean up old data from any previous tesets conn-&gt;remove(ns, BSONObj()); verify(conn-&gt;findOne(ns, BSONObj()).isEmpty()); //verify逻辑表达式，判断是否还存在test.first，是否clenn up成功 //test insert cout &lt;&lt; &quot;(1) test insert----&quot; &lt;&lt; endl; conn-&gt;insert(ns, BSON(&quot;name&quot; &lt;&lt; &quot;joe&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;123456&quot; &lt;&lt; &quot;age&quot; &lt;&lt; 20)); verify(!conn-&gt;findOne(ns, BSONObj()).isEmpty()); //判断是否添加成功 cout &lt;&lt; &quot;insert data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; cout &lt;&lt; &quot;insert success!&quot; &lt;&lt; endl; // test remove cout &lt;&lt; &quot;(2) test remove----&quot; &lt;&lt; endl; conn-&gt;remove(ns, BSONObj()); verify(conn-&gt;findOne(ns, BSONObj()).isEmpty()); cout &lt;&lt; &quot;remove success!&quot; &lt;&lt; endl; // insert, findOne testing conn-&gt;insert(ns, BSON(&quot;name&quot; &lt;&lt; &quot;joe&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;234567&quot; &lt;&lt; &quot;age&quot; &lt;&lt; 21)); { BSONObj res = conn-&gt;findOne(ns, BSONObj()); verify(strstr(res.getStringField(&quot;name&quot;), &quot;joe&quot;)); verify(!strstr(res.getStringField(&quot;name2&quot;), &quot;joe&quot;)); verify(21 == res.getIntField(&quot;age&quot;)); } cout &lt;&lt; &quot;insert data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt;endl; // test update { BSONObj res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); verify(!strstr(res.getStringField(&quot;name2&quot;), &quot;jeo&quot;)); BSONObj after = BSONObjBuilder().appendElements(res).append(&quot;name2&quot;, &quot;w&quot;).obj(); cout &lt;&lt; &quot;(3) test update name joe add name2 name3----&quot; &lt;&lt; endl; //upsert type1 update method conn-&gt;update(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj(), after); //res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name2&quot;, &quot;w&quot;).obj()); verify(!strstr(res.getStringField(&quot;name2&quot;), &quot;joe&quot;)); verify(conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe2&quot;).obj()).isEmpty()); //cout &lt;&lt; &quot; update1 data: &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; cout &lt;&lt; &quot; update1 data : &quot; &lt;&lt; conn-&gt;findOne(ns, Query(&quot;{name2:'w'}&quot;)) &lt;&lt; endl; //upsert type2 update method 更改指定的某个属性值 const string TEST_NS = &quot;test.first&quot;; conn-&gt;update(&quot;test.first&quot;, res, BSON(&quot;$set&quot; &lt;&lt; BSON(&quot;age&quot; &lt;&lt; 11))); cout &lt;&lt; &quot; update2 data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); //upsert type3 update method try { after = BSONObjBuilder().appendElements(res).append(&quot;name3&quot;, &quot;h&quot;).obj(); conn-&gt;update(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj(), after); } catch (OperationException&amp;) { cout &lt;&lt; &quot; update error: &quot; &lt;&lt; conn-&gt;getLastErrorDetailed().toString() &lt;&lt; endl; } verify(!conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()).isEmpty()); //cout &lt;&lt; &quot; update3 data: &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; cout &lt;&lt; &quot; update3 data : &quot; &lt;&lt; conn-&gt;findOne(ns, Query(&quot;{name3:'h'}&quot;)) &lt;&lt; endl; cout &lt;&lt; &quot;(4) test query-----&quot; &lt;&lt; &quot;\\n query data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()) &lt;&lt; endl; cout &lt;&lt; &quot; Query data : &quot; &lt;&lt; conn-&gt;findOne(ns, Query(&quot;{name:'joe'}&quot;)) &lt;&lt; endl; } return 0;} (2) makefile文件 123456789101112CC = g++TEST = simple_client_test$(TEST): $(CC) $@.cpp -pthread -lmongoclient -lboost_thread -lboost_system -lboost_regex -o $@ ./$(TEST) rm -f $(TEST)run: ./$(TEST)clean: rm -f $(TEST)","link":"/2016/11/10/mongodb%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--C++%E6%93%8D%E4%BD%9Cmongodb/"},{"title":"mongodb学习笔记--C++操作mongodb","text":"在学习mongodb过程当中，必须学习的就是用C++（Java、PHP、C#等）操作mongodb，这里讲述C++操作mongodb，在官方提供的mongo-cxx-driver驱动中有相关的操作例子，可以结合例子学习，目录是mongo-cxx-driver-legacy-1.0.0-rc0\\src\\mongo\\client\\examples，这里针对几个重要的点讲述。 1. C++连接mongodb（1）有无密码都适用 123456789101112131415161718192021mongo::client::GlobalInstance instance;if (!instance.initialized()) { std::cout &lt;&lt; &quot;failed to initialize the client driver: &quot; &lt;&lt; instance.status() &lt;&lt; std::endl; return EXIT_FAILURE;}std::string uri = &quot;mongodb://username:password@127.0.0.1:27017&quot;;std::string errmsg;ConnectionString cs = ConnectionString::parse(uri, errmsg);if (!cs.isValid()) { std::cout &lt;&lt; &quot;Error parsing connection string &quot; &lt;&lt; uri &lt;&lt; &quot;: &quot; &lt;&lt; errmsg &lt;&lt; std::endl; return EXIT_FAILURE;}boost::scoped_ptr&lt;DBClientBase&gt; conn(cs.connect(errmsg));if (!conn) { cout &lt;&lt; &quot;couldn't connect : &quot; &lt;&lt; errmsg &lt;&lt; endl; return EXIT_FAILURE;} 这种方式有密码和无密码的情况都适用，&quot;username&quot;为登录名，&quot;password&quot;为登录密码；无密码时去掉&quot;username:password@&quot;即可连接，这样连接方式比较常用。 （2）常规连接操作（不带密码连接） 1234567891011mongo::DBClientConnection conn; mongo::Status status = mongo::client::initialize();if (!status.isOK()) { MongoException m(-1, &quot;failed to initialize the client driver: &quot; + status.toString()); return -1;}string url = &quot;localhost:27017&quot;;if (!conn.connect(url, errmsg)) { MongoException m(0, &quot;couldn’t connect : &quot; + errmsg); return -1;} 通过这种方式可建立与mongodb的连接，但是是未带权限的连接。 （3）带密码连接 1conn.auth( &quot;admin&quot; , &quot;username&quot; , &quot;password&quot; , errmsg ); 其中&quot;admin&quot;为验证用户的所在数据库，一般在&quot;test.system.users&quot;或&quot;admin.system.users&quot;表中，所以填库名&quot;test&quot;或&quot;admin&quot;，&quot;username&quot;为登录名，&quot;password&quot;为登录密码。或者下面的auth方式也可以： 1234conn.auth(BSON( &quot;user&quot; &lt;&lt; &quot;root&quot; &lt;&lt; &quot;db&quot; &lt;&lt; &quot;admin&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;password&quot; &lt;&lt; &quot;mechanism&quot; &lt;&lt; &quot;DEFAULT&quot;)); 根据mongodb的设置更改传递给函数conn.auth的参数值。 2. C++查询数据123const char* ns = &quot;test.first&quot;;conn-&gt;findOne(ns, BSONObj()); //查询一条数据conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); //条件查询 3. C++插入数据1234const char* ns = &quot;test.first&quot;;conn-&gt;insert(ns, BSON(&quot;name&quot; &lt;&lt; &quot;joe&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;123456&quot; &lt;&lt; &quot;age&quot; &lt;&lt; 20)); 4. C++删除数据123const char* ns = &quot;test.first&quot;;conn-&gt;remove(ns, BSONObj()); //删除所有conn-&gt;remove(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); //删除指定文档 5. C++更新数据(1) 在原数据基础上增加属性 1234const char* ns = &quot;test.first&quot;;BSONObj res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj());BSONObj after = BSONObjBuilder().appendElements(res).append(&quot;name2&quot;, &quot;h&quot;).obj();conn-&gt;update(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;jeo2&quot;).obj(), after); //在原数据上增加属性 这种update方式只是在原数据基础上增加属性，并不会改变指定属性值。(2) 更改指定的某个属性值**想要update指定属性的值，需要用到”$set”指令**，如下代码所示： 123BSONObj res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj());//更改指定的某个属性值conn-&gt;update(&quot;test.first&quot;, res, BSON(&quot;$set&quot; &lt;&lt; BSON(&quot;age&quot; &lt;&lt; 11))); 主要想强调下update更改指定的某个属性值的方式，比较常用。 6. C++操作例子(1) simple_client_test.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127/*#if defined(_WIN32)#include &lt;winsock2.h&gt;#include &lt;windows.h&gt;#endif*///g++ simple_client_test.cpp -pthread -lmongoclient -lboost_thread -lboost_system -lboost_regex -o simple_client_test#include &quot;mongo/client/dbclient.h&quot; // the mongo c++ driver#include &lt;iostream&gt;#ifndef verify#define verify(x) MONGO_verify(x)#endifusing namespace std;using namespace mongo;//using mongo::BSONObj;//using mongo::BSONObjBuilder;int main(int argc, char* argv[]) { if (argc &gt; 2) { std::cout &lt;&lt; &quot;usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; [MONGODB_URI]&quot; &lt;&lt; std::endl; return EXIT_FAILURE; } mongo::client::GlobalInstance instance; if (!instance.initialized()) { std::cout &lt;&lt; &quot;failed to initialize the client driver: &quot; &lt;&lt; instance.status() &lt;&lt; std::endl; return EXIT_FAILURE; } std::string uri = argc == 2 ? argv[1] : &quot;mongodb://127.0.0.1:27017&quot;; std::string errmsg; ConnectionString cs = ConnectionString::parse(uri, errmsg); if (!cs.isValid()) { std::cout &lt;&lt; &quot;Error parsing connection string &quot; &lt;&lt; uri &lt;&lt; &quot;: &quot; &lt;&lt; errmsg &lt;&lt; std::endl; return EXIT_FAILURE; } boost::scoped_ptr&lt;DBClientBase&gt; conn(cs.connect(errmsg)); if (!conn) { cout &lt;&lt; &quot;couldn't connect : &quot; &lt;&lt; errmsg &lt;&lt; endl; return EXIT_FAILURE; } const char* ns = &quot;test.first&quot;; conn-&gt;dropCollection(ns); //clean up old data from any previous tesets conn-&gt;remove(ns, BSONObj()); verify(conn-&gt;findOne(ns, BSONObj()).isEmpty()); //verify逻辑表达式，判断是否还存在test.first，是否clenn up成功 //test insert cout &lt;&lt; &quot;(1) test insert----&quot; &lt;&lt; endl; conn-&gt;insert(ns, BSON(&quot;name&quot; &lt;&lt; &quot;joe&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;123456&quot; &lt;&lt; &quot;age&quot; &lt;&lt; 20)); verify(!conn-&gt;findOne(ns, BSONObj()).isEmpty()); //判断是否添加成功 cout &lt;&lt; &quot;insert data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; cout &lt;&lt; &quot;insert success!&quot; &lt;&lt; endl; // test remove cout &lt;&lt; &quot;(2) test remove----&quot; &lt;&lt; endl; conn-&gt;remove(ns, BSONObj()); verify(conn-&gt;findOne(ns, BSONObj()).isEmpty()); cout &lt;&lt; &quot;remove success!&quot; &lt;&lt; endl; // insert, findOne testing conn-&gt;insert(ns, BSON(&quot;name&quot; &lt;&lt; &quot;joe&quot; &lt;&lt; &quot;pwd&quot; &lt;&lt; &quot;234567&quot; &lt;&lt; &quot;age&quot; &lt;&lt; 21)); { BSONObj res = conn-&gt;findOne(ns, BSONObj()); verify(strstr(res.getStringField(&quot;name&quot;), &quot;joe&quot;)); verify(!strstr(res.getStringField(&quot;name2&quot;), &quot;joe&quot;)); verify(21 == res.getIntField(&quot;age&quot;)); } cout &lt;&lt; &quot;insert data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt;endl; // test update { BSONObj res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); verify(!strstr(res.getStringField(&quot;name2&quot;), &quot;jeo&quot;)); BSONObj after = BSONObjBuilder().appendElements(res).append(&quot;name2&quot;, &quot;w&quot;).obj(); cout &lt;&lt; &quot;(3) test update name joe add name2 name3----&quot; &lt;&lt; endl; //upsert type1 update method conn-&gt;update(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj(), after); //res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name2&quot;, &quot;w&quot;).obj()); verify(!strstr(res.getStringField(&quot;name2&quot;), &quot;joe&quot;)); verify(conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe2&quot;).obj()).isEmpty()); //cout &lt;&lt; &quot; update1 data: &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; cout &lt;&lt; &quot; update1 data : &quot; &lt;&lt; conn-&gt;findOne(ns, Query(&quot;{name2:'w'}&quot;)) &lt;&lt; endl; //upsert type2 update method 更改指定的某个属性值 const string TEST_NS = &quot;test.first&quot;; conn-&gt;update(&quot;test.first&quot;, res, BSON(&quot;$set&quot; &lt;&lt; BSON(&quot;age&quot; &lt;&lt; 11))); cout &lt;&lt; &quot; update2 data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; res = conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()); //upsert type3 update method try { after = BSONObjBuilder().appendElements(res).append(&quot;name3&quot;, &quot;h&quot;).obj(); conn-&gt;update(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj(), after); } catch (OperationException&amp;) { cout &lt;&lt; &quot; update error: &quot; &lt;&lt; conn-&gt;getLastErrorDetailed().toString() &lt;&lt; endl; } verify(!conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()).isEmpty()); //cout &lt;&lt; &quot; update3 data: &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObj()) &lt;&lt; endl; cout &lt;&lt; &quot; update3 data : &quot; &lt;&lt; conn-&gt;findOne(ns, Query(&quot;{name3:'h'}&quot;)) &lt;&lt; endl; cout &lt;&lt; &quot;(4) test query-----&quot; &lt;&lt; &quot;\\n query data : &quot; &lt;&lt; conn-&gt;findOne(ns, BSONObjBuilder().append(&quot;name&quot;, &quot;joe&quot;).obj()) &lt;&lt; endl; cout &lt;&lt; &quot; Query data : &quot; &lt;&lt; conn-&gt;findOne(ns, Query(&quot;{name:'joe'}&quot;)) &lt;&lt; endl; } return 0;} (2) makefile文件 123456789101112CC = g++TEST = simple_client_test$(TEST): $(CC) $@.cpp -pthread -lmongoclient -lboost_thread -lboost_system -lboost_regex -o $@ ./$(TEST) rm -f $(TEST)run: ./$(TEST)clean: rm -f $(TEST)","link":"/2016/11/10/mongodb%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0C++%E6%93%8D%E4%BD%9Cmongodb/"},{"title":"mongodb学习笔记--$elemMatch操作符的使用","text":"mongodb提供了很多操作符以便更加方便快捷地操作数据，下面我们来认识一下查询操作符$elemMatch，$elemMatch投影操作符将限制查询返回的数组字段的内容只包含匹配$elemMatch条件的数组元素。 注意： 数组中元素是内嵌文档。 如果多个元素匹配$elemMatch条件，操作符返回数组中第一个匹配条件的元素。 1.首先创建一个简单文档1db.test.insert({&quot;id&quot;:1, &quot;members&quot;:[{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:27, &quot;gender&quot;:&quot;M&quot;}, {&quot;name&quot;:&quot;BuleRiver2&quot;, &quot;age&quot;:23, &quot;gender&quot;:&quot;F&quot;}, {&quot;name&quot;:&quot;BuleRiver3&quot;, &quot;age&quot;:21, &quot;gender&quot;:&quot;M&quot;}]}); 2.使用多种方式尝试查询(1) 使用db.test.find({&quot;members&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;}});进行查询： 1db.test.find({&quot;members&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;}}); 查询的结果是空集。 (2) 只有完全匹配一个的时候才能获取到结果，因此： 1db.test.find({&quot;members&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:27, &quot;gender&quot;:&quot;M&quot;}}); 可以得到结果。 (3) 如果把键值进行颠倒,也得不到结果： 1db.test.find({&quot;members&quot;:{&quot;age&quot;:27, &quot;name&quot;:&quot;BuleRiver1&quot;, &quot;gender&quot;:&quot;M&quot;}}); 得到的结果是空集。 (4) 我们这样查询： 1db.test.find({&quot;members.name&quot;:&quot;BuleRiver1&quot;}); 是可以查询出结果的。 (5) 如果需要两个属性： 1db.test.find({&quot;members.name&quot;:&quot;BuleRiver1&quot;, &quot;members.age&quot;:27}); 也可以查询出结果。 (6) 我们再进行破坏性尝试： 1db.test.find({&quot;members.name&quot;:&quot;BuleRiver1&quot;, &quot;members.age&quot;:23}); 也可以查询出结果。 不过我们应该注意到：BuleRiver1是数组中第一个元素的键值，而23是数组中第二个元素的键值，这样也可以查询出结果。 3.使用$elemMatch操作符查询 对于我们的一些应用来说，以上结果显然不是我们想要的结果。所以我们应该使用$elemMatch操作符: (1)``$elemMatch+同一个元素中的键值组合 1db.test.find({&quot;members&quot;:{&quot;$elemMatch&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:27}}}); 可以查询出结果； (2)$elemMatch+不同元素的键值组合 1db.test.find({&quot;members&quot;:{&quot;$elemMatch&quot;:{&quot;name&quot;:&quot;BuleRiver1&quot;, &quot;age&quot;:23}}}); 查询不出结果。 因此，(1)展示的嵌套查询正是我们想要的查询方式。","link":"/2017/02/21/mongodb%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0elemMatch%E6%93%8D%E4%BD%9C%E7%AC%A6%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"mongodb学习笔记--分页查询优化","text":"有时候我们在非关系数据库mongodb做一些简单的分析查询，比如分页。mongodb本身提供了分页的api，但是比较有限。 1、分页apimongodb自身提供了类似mysql的分页关键字： 查询时 1query.skip((pageNum - 1)*pageSize).limit(pageSize); 需要是跳跃查询，但是达到一定数据量级的，查询效率很低，甚至查不动。 skip达到一定数据量时，超过了系统默认的32MB内存排序，所以mongo重新使用了其他算法排序，出现大量扫描，导致慢查询。 这个写法和mysql的limit offset,rows类似： 下面两条语句是等价的。 123select id from collect limit 1000000,10;#**MySQL5.0之后支持该语法**select id from collect limit 10 offset 1000000; 等于是一直往后读取到第1000000条，开始取10条，读取了1000000没用的数据，mysql的优化方式是找到第1000000条数据的id，开始读取10条，主要是利用索引位置来定位分页起始位： 1select id from collect where id&gt;1000000 limit 10; 查询过程经过了pageNum*pageSize条数据。 1234567graph LRweb--&gt;pageNumweb--&gt;pageSizepageNum--&gt;pageNum*pageSizepageSize--&gt;pageNum*pageSizepageNum*pageSize--&gt;skipskip--&gt;limit第pageNum页的数据 2、优化方向等官方优化skip()确定时间，毕竟mongo使用场景也大多不是用来分页，那么只能避免使用skip()。不使用skip()那怎么优化呢？我们借助其他函数曲线救国，比如sort()排序和 limit() ，和mysql的优化方向一样，知道起始id再查询当前页，理论上会快很多。相当于跳过了pageNum*pageSize条数据。 123456789graph LRweb--&gt;pageNumweb--&gt;pageSizepageNum--&gt;pageNum*pageSize的数据_idpageSize--&gt;pageNum*pageSize的数据_idpageNum*pageSize的数据_id--&gt;使用$gt函数大于_idpageNum*pageSize的数据_id--&gt;skip跳页pageSize使用$gt函数大于_id--&gt;第pageNum页的数据skip跳页pageSize--&gt;第pageNum页的数据 3、实现对象中设置id属性即可得到_id的值，String的话得到一串十六进制的字符构成的字符串，具体构成可查看ObjectId的构成。 假设实体对象为Goods，设置id属性： 1private String id; 将得到_id设置为分页的起始_id，，具体实现如下： 123456789101112131415161718192021222324252627282930public List&lt;Goods&gt; getListByPage(int pageNum, int pageSize) { List&lt;Goods&gt; goodsList; Query query = new Query(); // 通过 _id 正序排序 query.with(Sort.by(Sort.Direction.ASC, &quot;_id&quot;)); if (pageNum != 1) { // number 参数是为了查上一页的最后一条数据 int number = (pageNum - 1) * pageSize; query.limit(number); List&lt;Goods&gt; goodsListTemp = mongoTemplate.find(query, Goods.class); // 取出最后一条 Goods goods = goodsListTemp.get(goodsListTemp.size() - 1); // 取到上一页的最后一条数据_id，当作条件查接下来的数据 String id = goods.getId(); // 从上一页最后一条开始查（大于不包括这一条） query.addCriteria(Criteria.where(&quot;_id&quot;).gt(id)); } // 页大小重新赋值，覆盖 number 参数 query.limit(pageSize); // 即可得到第n页数据 goodsList = mongoTemplate.find(query, Goods.class); return goodsList;} 这样的避免了skip()的使用，通过sort()排序和 limit() 限制数据大小结合排序，每一次分页查询从数据的上一页的最后一条数据作为起始位置，再查询页大小的数据量。 4、建议从代码表现上来看，每一次查询都需要通过查询当前页之前的所有数据来得到起始位置的_id，相当于查询了大量数据，对内存消耗较大。 针对页数不是特别多的情况下，这种优化方式也是比skip()效率要高的。 如果页数特别多，每页size也较大，那么不管是什么类型的数据库压力也是比较大的，这种情况就需要从业务方面考量，不适合做分页，或者将分析型业务独立，这样的分析统计类操作可以放到Elasticsearch等更适合的存储上。","link":"/2020/09/13/mongodb%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/"},{"title":"mongodb学习笔记--实现自增ID序列","text":"在某些特定的场景下，我们可能需要利用数据的自增id特性来作为数据集的自增排序查找等操作。或者相同的数据分布在多份存储上。 并且存在对条数据需要通过这个id进行关联，类似于外键特性，这就需要将id和数据分离。 流程： 12345graph LRid--&gt;DB1id--&gt;DB2DB1--&gt;合并数据DB2--&gt;合并数据 这里列举了mysql、oracle、mongodb三种数据的自增序列实现方式。 1、mysql的自增id​ mysql的自增id是和数据绑定在一起的，无法分开，等于你id的增加是伴随着数据库的数据新增而来。 那有没有办法分开呢？ ​ 也是有的，需要借助数据库函数来实现。但是一般不建议使用，需要对函数进行分析加参数修饰，如函数里加关键字READS SQL DATA（只查询时使用）、DETERMINISTIC（存在更新时使用），不然数据库会告警，会一定程度影响dml性能。 其原理就是每查询一次更新sequence的值。 具体实现步骤如下： 123456789101112131415161718192021222324252627282930313233343536373839--1、新建序列表drop table if exists test_sequence;create table test_sequence (seq_name VARCHAR(50) NOT NULL, -- 序列名称current_val bigint NOT NULL, -- 当前值increment_val bigint NOT NULL DEFAULT 1, -- 步长(跨度)PRIMARY KEY (seq_name)); --2、新增一个序列INSERT INTO test_sequence VALUES ('user_seq', '0', '1');INSERT INTO test_sequence VALUES ('class_seq', '0', '1');INSERT INTO test_sequence VALUES ('school_seq', '0', '1');INSERT INTO test_sequence VALUES ('data_seq', '0', '1'); --3、创建currval函数，用于获取序列当前值create function seq_currval(v_seq_name VARCHAR(50))returns bigint(20)begindeclare value integer;set value = 0;select current_val into value from test_sequence where seq_name = v_seq_name;return value;end; --4、查询当前值select seq_currval('ge_rns_user_info_seq'); --5、创建nextval函数，用于获取序列下一个值create function seq_nextval (v_seq_name VARCHAR(50)) returns bigint(20)beginupdate test_sequence set current_val = current_val + increment_val where seq_name = v_seq_name;return seq_currval(v_seq_name);end; --6、查询下一个值select seq_nextval('user_seq');--7、查询所有sequenceselect * from test_sequence; 使用 2、oracle的自增sequenceoracle的自增sequence是独立于数据的，对于迁移维护比较方便。具体使用如下： 1234567891011121314151617181920--1、首先建一个表TESTcreate table TEST( NID int PRIMARY KEY, test1 varchar2(20), test2 varchar2(20), test3 varchar2(20), test4 varchar2(20), test5 varchar2(20))--2、再建一个序列SEQ_TESTcreate sequence SEQ_TESTminvalue 1 --最小值nomaxvalue --不设置最大值start with 1 --从1开始计数increment by 1 --每次加1个nocycle --一直累加，不循环nocache; --不建缓冲区 然后在项目的mybatis中使用下面的语句赋值到数据id上即可； 1SELECT SEQ_TEST FROM DUAL 就是这么的简单，so easy! 3、mongo实现自动增长sequence严格来说mongodb本身没有自增序列相关的功能，但是我们了解到mongodb提供了原子操作的函数$inc，既然是原子操作，那么就可以在这上面做文章，自增id也是全局唯一，那么有了原子操作，我们的自增操作就能够满足。在每次进行插入操作的时候，序列值加1，作为本次操作的id。 具体实现如下： 1、定义sequence对象1234567891011121314151617181920212223import org.springframework.data.annotation.Id;public class MongoSequence { @Id private String id; private long nextVal; public String getId() { return id; } public void setId(String id) { this.id = id; } public long getNextVal() { return nextVal; } public void setNextVal(long nextVal) { this.nextVal = nextVal; }} 2、创建自增序列工具类取号工具类MongoAutoIdUtil.java： 1234567891011121314@Componentpublic class MongoAutoIdUtil { @Autowired private MongoTemplate mongoTemplate; public long getNextSequenceByCol(String collectionName) { MongoSequence sequence = mongoTemplate.findAndModify(Query.query(where(&quot;_id&quot;).is(collectionName)), new Update().inc(&quot;nextVal&quot;, 1L), options().upsert(true).returnNew(true), MongoSequence.class); return sequence.getNextVal(); }} 有没有发现这里的update操作并没有集合名称，那么具体是更新哪个集合呢？会不会失败？ 别担心，如果不设置集合的话，执行的时候mongodb自己会新增一个默认集合名字叫mongoSequence，所以我们只需要调用这个方法，传入我们想要的序列名即可。然后在mongodb控制台中就能看到mongoSequence集合中存在了我们想要的需要，每次只要调用这个方法获取的就是自增的id。 3、函数性能那这个函数的性能怎样呢？ 性能的话，我稍微测试了下，50~200个并发取id没有出现id重复的情况，并且自增无误。再高的并发本机4核cpu太弱，测试就不再具有参考性，就作罢了。 官方的解释是inc与set性能相当，比$push快。 在实际运用过程当中也没有出现重复取和自增出错的情况，运行良好。 不过既然选择了mongodb，那么最好利用非关系数据库的特性，发挥它最大的性能，不要将自增id这样的场景放在mongodb上操作。 比如它本身的_id其实就是个很好的全局唯一并且支持排序的序列，当然我们也可以对_id进行赋值，设置我们自己的数据类型，比如设置为上面的自增id的数字，但是不推荐。 官方针对_id的有解释，少量数据量的情况下，对_id进行赋值和不赋值性能差别不大，但达到一定数据量的时候，_id自动赋值比手动赋值性能要好，因为需要额外对我们手动赋的值做一次多分片全局唯一性的检查操作，这没有mongodb自己的生成算法来得快。","link":"/2020/09/25/mongodb%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%A2%9EID%E5%BA%8F%E5%88%97/"},{"title":"mybatis中的#{}和${}的区别","text":"在使用mybatis时都会用到#{}和${}符号来进行传值，我们来看下两者的区别。 1. 特征 #{}将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：order by #user_id#，如果传入的值是111,那么解析成sql时的值为order by &quot;111&quot;, 如果传入的值是id，则解析成的sql为order by &quot;id&quot; 。 ${}将传入的数据直接显示生成在sql中。如：order by $user_id$，如果传入的值是111,那么解析成sql时的值为order by 111, 如果传入的值是id，则解析成的sql为order by id. #{}方式能够很大程度防止sql注入。 ${}方式无法防止Sql注入。 ${}方式一般用于传入数据库对象，例如传入表名。 一般能用#{}的就别用${} 。 mybatis排序时使用order by 动态参数时需要注意，用${}而不是#{} 2. 字符串替换默认情况下，使用#{}格式的语法会导致mybatis创建预处理语句属性并以它为背景设置安全的值（比如?）。这样做很安全，很迅速也是首选做法，有时你只是想直接在SQL语句中插入一个不改变的字符串。比如，像ORDER BY，你可以这样来使用： 1ORDER BY ${columnName} 这里mybatis不会修改或转义字符串。 重要：接受从用户输出的内容并提供给语句中不变的字符串，这样做是不安全的。这会导致潜在的SQL注入攻击，因此你不应该允许用户输入这些字段，或者通常自行转义并检查。 3. mybatis 本身的说明12345678String SubstitutionBy default, using the #{} syntax will cause MyBatis to generate PreparedStatement properties and set the values safely against the PreparedStatement parameters (e.g. ?). While this is safer, faster and almost always preferred, sometimes you just want to directly inject a string unmodified into the SQL Statement. For example, for ORDER BY, you might use something like this:ORDER BY ${columnName}Here MyBatis won't modify or escape the string.NOTE It's not safe to accept input from a user and supply it to a statement unmodified in this way. This leads to potential SQL Injection attacks and therefore you should either disallow user input in these fields, or always perform your own escapes and checks. 从上文可以看出： 使用#{}格式的语法在mybatis中使用Preparement语句来安全的设置值，执行sql类似下面的： 12PreparedStatement ps = conn.prepareStatement(sql);ps.setInt(1,id); 这样做的好处是：更安全，更迅速，通常也是首选做法。 不过有时你只是想直接在 SQL 语句中插入一个不改变的字符串。比如，像 ORDER BY，你可以这样来使用： 1ORDER BY ${columnName} 此时mybatis 不会修改或转义字符串。 这种方式类似于： 12Statement st = conn.createStatement();ResultSet rs = st.executeQuery(sql); 这种方式的缺点是： 以这种方式接受从用户输出的内容并提供给语句中不变的字符串是不安全的，会导致潜在的 SQL 注入攻击，因此要么不允许用户输入这些字段，要么自行转义并检验。","link":"/2017/04/20/mybatis_operator/"},{"title":"zookeeper提示Unable to read additional data from server sessionid 0x","text":"配置zookeeper集群，一开始配置了两台机器server.1和server.2。 配置参数，在zoo.cfg中指定了整个zookeeper集群的server编号、地址和端口： server.1=10.10.16.151:2888:3888server.2=10.10.16.234:2888:3888 然后为这两个个节点创建对应的编号文件，在/tmp/zookeeper/data/myid文件中。如下： 在server.1=10.10.16.151机器上执行： 1echo 1 &gt; /tmp/zookeeper/data/myid 在server.1=10.10.16.234机器上执行： 1echo 2 &gt; /tmp/zookeeper/data/myid 启动server.1测试dubbo服务，使用zkServer.sh start启动了server.1，然后使用zkServer.sh status查看工作状态，显示 1Error contacting service. It is probably not running. 通过zkCli.sh -server 10.10.16.151:2181查看服务详细，发现服务提示了下面的错误信息。 启动时提示： 1232017-09-15 14:57:27,139 [myid:] - INFO [main-SendThread(10.10.16.151:2181):ClientCnxn$SendThread@1035] - Opening socket connection to server 10.10.16.151/10.10.16.151:2181. Will not attempt to authenticate using SASL (java.lang.SecurityException: Ϟ·¨¶¨λµȂ¼Ƥ׃)2017-09-15 14:57:27,140 [myid:] - INFO [main-SendThread(10.10.16.151:2181):ClientCnxn$SendThread@877] - Socket connection established to 10.10.16.151/10.10.16.151:2181, initiating session2017-09-15 14:57:27,143 [myid:] - INFO [main-SendThread(10.10.16.151:2181):ClientCnxn$SendThread@1161] - Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect 后来才搞明白，由于我在zoo.cfg中配置了2台机器，但是只启动了1台，zookeeper就会认为服务处于不可用状态。 通过zookeeper的选举算法得知，当整个集群超过半数机器宕机，zookeeper会认为集群处于不可用状态。所以启动2台服务正常。 然后我又增加了1台server.3=10.10.16.241:2888:3888机器节点，在3台都启动的情况下，关掉其中1台，服务正常，关掉2台，服务不可用。 所以，zookeeper集群只启动一台无法连接，如果启动机器数为半数及以上就可以连接了。","link":"/2017/09/15/zookeeper%E6%8F%90%E7%A4%BAUnable%20to%20read%20additional%20data%20from%20server%20sessionid%200x/"},{"title":"关于&#x3D;null和clear()问题（Java性能优化）","text":"以ArrayList为例，根据情况来看吧，ArrayList内部维护的是一个数组。 1. list = null 那么你list = null; ** 就是释放这个数组对象，当然里面所引用的对象也就释放了**。 2. list.clear() 如果list.clear(); 看看源代码就知道了，是把list里面对象遍历赋值为null，意思就是释放list里面所有对象。 这样就很清楚了，如果你这个list还需要使用，那么你可以使用clear去释放掉list里面所有的数据，GC在回收的时候就有可能回收掉原list里面的这些数据了。如果你都不需要了，那么你就用=null这样GC就有可能把list以及里面关联的一些数据也都回收了。 3. 咱们来看下clear()源码clear()源码解析：If we take an ArrayList as an example, the clear() method does this: 123456789public void clear() { modCount++; // Let gc do its work for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; } Basically, if elements of a List are not referenced anywhere else in the code there is really no need (or at least you do not gain anything) to call clear(). You also do not need to assign null to the List because it will be garbage collected as soon as it falls out of scope.译文： 基本上，如果List中的元素没有被引用到代码中的其他地方，那么确实没有必要（或至少你没有获得任何东西）来调用clear()。 您也不需要为List指定null，因为它会在超出范围的情况下被垃圾回收。","link":"/2016/04/19/%E5%85%B3%E4%BA%8E=null%E5%92%8Cclear()%E9%97%AE%E9%A2%98(Java%E6%80%A7%E8%83%BD%E7%AF%87)/"},{"title":"前后端分离，前端跨域访问后台的两种方式","text":"​ 在我们做web项目项目时，前端和后端都是不同的开发人员负责，到某个时间点必然要进行前后端数据交互，这时候就会遇到跨域问题。这里介绍两种解决方式。 方式1：如果开发是tomcat服务器，在工程的web.xml添加设置​ 第一种方式是通过CORS (Cross-Origin Resource Sharing)跨域资源共享的方式实现。 ​ 在web.xml文件中设置CORS内容，如果后台开发人用的是tomcat服务器的话，则在web工程的web.xml文件中添加一下内容即可： 1234567891011121314151617181920212223242526272829&lt;!--解决tomcat部署跨域问题 --&gt;&lt;filter&gt; &lt;filter-name&gt;CORS&lt;/filter-name&gt; &lt;filter-class&gt;com.thetransactioncompany.cors.CORSFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;cors.allowOrigin&lt;/param-name&gt; &lt;param-value&gt;*&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;cors.supportedMethods&lt;/param-name&gt; &lt;param-value&gt;GET, POST, HEAD, PUT, DELETE&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;cors.supportedHeaders&lt;/param-name&gt; &lt;param-value&gt;Accept, Origin, X-Requested-With, Content-Type, Last-Modified&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;cors.exposedHeaders&lt;/param-name&gt; &lt;param-value&gt;Set-Cookie&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;cors.supportsCredentials&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CORS&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 这是通过CORS (Cross-Origin Resource Sharing)跨域资源共享的方式实现。这种方式对chrome浏览器调试有点问题，firefox及IE调试正常。 方式2：通过ajax和过滤器实现跨域​ 第二种方式是通过往请求消息头中加Access-Control-Allow-Origin。 ​ Access-Control-Allow-Origin是HTML5 添加的新功能，设置这个属性值为你前端所在服务器的ip地址。 首先后台添加过滤器AccessFilter.java： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.io.IOException;import javax.servlet.Filter;import javax.servlet.FilterChain;import javax.servlet.FilterConfig;import javax.servlet.ServletException;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletResponse;import org.springframework.beans.factory.annotation.Value;/** * @Title: AccessFilter.java * @Description: TODO(设置其他IP地址的机器可以直接访问本项目Url--工具filter) */public class AccessFilter extends HttpServlet implements Filter{ /** * 序列ID，用来监测控制版本更新的 */ private static final long serialVersionUID = 1L; @Value(&quot;${test.mavenspring.project.allow.url}&quot;) private String projectDemoAllowOriginUrl; @Override public void init(FilterConfig filterConfig) throws ServletException { } //设置其他IP地址的机器可以直接访问到这个项目的后端 @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { HttpServletResponse httpResponse = (HttpServletResponse) response; httpResponse.setHeader(&quot;Access-Control-Allow-Origin&quot;, projectDemoAllowOriginUrl); httpResponse.setHeader(&quot;Access-Control-Allow-Headers&quot;,&quot;Origin, X-Requested-With, Content-Type, Accept&quot;); httpResponse.setHeader(&quot;Access-Control-Allow-Credentials&quot;, &quot;true&quot;); chain.doFilter(request, httpResponse); }} 然后在前端js的ajax部分按照下面的方式设置： 1234567891011$.ajaxSettings.xhrFields = { withCredentials : true};$.ajax({ crossDomain : true, type : 'post', url : 'http://...', dataType : 'json', data : '...', //其他处理不变}); 默认情况下widthCredentials为false，我们需要设置widthCredentials为true。 有一点需要注意，设置了widthCredentials为true的请求中会包含远程域的所有cookie，但这些cookie仍然遵循同源策略，所以你是访问不了这些cookie的。以上是我在实际项目中用到的两种方式，第一种方式用chrome调试会有点问题，第二种方式兼容性较好，主流浏览器调试正常。 不过一定要记得，这个仅仅作为前后端交互调试之用， 开发完成后记得删除这些跨域部分， 避免项目上线之后出现安全问题。切记！","link":"/2017/02/28/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%EF%BC%8C%E5%89%8D%E7%AB%AF%E8%B7%A8%E5%9F%9F%E8%AE%BF%E9%97%AE%E5%90%8E%E5%8F%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/"},{"title":"恒大 VS 上港","text":"⚽很少看中超，三线伪球迷，偶得一张恒大对战上港的票，亚冠联赛1/4决赛“中国德比”第二回合，非常值得一看。于是和峰哥守情瞪着小二轮儿就往天河体育中心赶。 ​ 首回合上港主场在上海，所以这一回合是恒大广州主场，第一回合上港4-0恒大，所以这一回恒大势必会攻势强劲。这回要5-0上港才能赢得这次比赛。 场外买了件阿兰的7号球衣。 比赛开始了。 头一回靠球迷协会这么近，整场球迷协会都在起势助威，感受到了他们真的很爱恒大，热爱足球。 我等十八线球迷也就是在进球时亢奋十秒，丢！ 整场下来，上港队队员受伤换下的球员不下4个，一个是部分球员状态不太好，二是恒大队员处于压力踢的太激进。 第90分钟，李学鹏头球破门，广州恒大4-0上海上港！ 球迷们又是一阵亢奋！我也特激动，感觉主场来个5-0就赢了，看来胜利不远了，那个激动啊，真切感受到了足球的魅力。 可是，最怕这个可是了。 第109分钟，上港队胡尔克主罚的爆射直钻网窝死角，广州恒大4-1上海上港！恒大压力很大，穆里奇被上港队员拉倒，被罚点球，高拉特主罚命中，5-1打平了，悬的心又放下了。 常规赛结束，比赛进入加时，5-1，阿兰进俩球，高拉特进3球。压力已经在恒大队这边了，加时赛5:1，总比分恒大5-5战平上港，未分胜负，所以呢，就到了精彩的点球大战。又是加时，又是点球，双方都紧咬比分。 不幸的是，点球上港队5罚5中，恒大队5罚4中，全场比赛结束，总比分上海上港10-9淘汰广州恒大。 一波三折，常规赛以为要输了，然后又赢了，加时加上点球，以为要赢了，结果还是输了。恒大球员的拼劲有目共睹，整场球真的很精彩，现场看球，不管是球员还是球迷带给人的都是特别感动激昂的感受，尽管广州很热，现场的热情完全掩盖了球场内的热浪。 最后回谢球迷。 恒大队，虽败犹荣！","link":"/2017/09/12/%E6%81%92%E5%A4%A7%E4%B8%BB%E4%B8%8A%E6%B8%AF%E5%AE%A2/"},{"title":"idea搭建springdata+mongodb+maven+springmvc","text":"今天我们来学习一下SpringData操作MongoDB。项目环境：IntelliJ IDEA2017+maven3.5.0+MongoDB 3.2+JDK1.7+spring4.3.8 推荐网站（适合学习各种知识的基础）：http://www.runoob.com/ mongo安装请参考：http://www.runoob.com/mongodb/mongodb-window-install.html 1. 创建maven工程首相创建maven工程，New project: 这里不使用idea自带maven插件，改用下载好的3.5.0版maven； 由于最近osChina的maven仓库挂掉，推荐大家使用阿里的镜像，速度快的飞起maven配置：D:\\apache-maven-3.5.0\\conf\\setting.xml中找到mirrors: 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 项目结构如下图，手动创建相关文件夹及文件，并设置文件夹的对应属性： 至此，maven工程创建完毕。 2. 访问mongodb数据方式这里dao与mongoDao分别为mongoDB的两种查询方式： (1) dao为JPA的查询方式（请参考springdataJPA）； (2) mongoDao使用mongoTemplate，类似于关系型数据库使用的jdbcTemplate。 3. 详细代码先看配置文件spring-context.xml为最基本的spring配置: 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 开启注解 --&gt; &lt;context:annotation-config/&gt; &lt;!--扫描service包嗲所有使用注解的类型--&gt; &lt;context:component-scan base-package=&quot;com.lewis.mongo&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; &lt;!-- 导入mongodb的配置文件 --&gt; &lt;import resource=&quot;spring-mongo.xml&quot;/&gt;&lt;/beans&gt; spring-web.xml为springmvc的基本配置: 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd&quot;&gt; &lt;!--配置springmvc--&gt; &lt;!--1.开启springmvc注解模式--&gt; &lt;context:annotation-config/&gt; &lt;!--简化配置： (1)主动注册DefaultAnnotationHandlerMapping,AnnotationMethodHandlerAdapter (2)提供一系列功能：数据绑定，数字和日期的format @NumberFormt @DataTimeFormat，xml json默认的读写支持--&gt; &lt;mvc:annotation-driven/&gt; &lt;!--servlet-mapping--&gt; &lt;!--2静态资源默认的servlet配置， （1）允许对静态资源的处理：js，gif （2）允许使用“/”做整体映射--&gt; &lt;!-- 容器默认的DefaultServletHandler处理 所有静态内容与无RequestMapping处理的URL--&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!--3:配置jsp 显示viewResolver--&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot;/&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/views/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;!-- 4自动扫描且只扫描@Controller --&gt; &lt;context:component-scan base-package=&quot;com.lewis.mongo.controller&quot;/&gt; &lt;!-- 定义无需Controller的url&lt;-&gt;view直接映射 --&gt; &lt;mvc:view-controller path=&quot;/&quot; view-name=&quot;redirect:/hi/hello&quot;/&gt;&lt;/beans&gt; spring-mongo.xml为mongo配置: 12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mongo=&quot;http://www.springframework.org/schema/data/mongo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/data/mongo http://www.springframework.org/schema/data/mongo/spring-mongo.xsd&quot;&gt; &lt;!-- 加载mongodb的属性配置文件 --&gt; &lt;context:property-placeholder location=&quot;classpath*:mongo.properties&quot;/&gt; &lt;!-- spring连接mongodb数据库的配置 --&gt; &lt;mongo:mongo-client replica-set=&quot;${mongo.hostport}&quot; id=&quot;mongo&quot;&gt; &lt;mongo:client-options connections-per-host=&quot;${mongo.connectionsPerHost}&quot; threads-allowed-to-block-for-connection-multiplier=&quot;${mongo.threadsAllowedToBlockForConnectionMultiplier}&quot; connect-timeout=&quot;${mongo.connectTimeout}&quot; max-wait-time=&quot;${mongo.maxWaitTime}&quot; socket-timeout=&quot;${mongo.socketTimeout}&quot;/&gt; &lt;/mongo:mongo-client&gt; &lt;!-- mongo的工厂，通过它来取得mongo实例,dbname为mongodb的数据库名，没有的话会自动创建 --&gt; &lt;mongo:db-factory id=&quot;mongoDbFactory&quot; dbname=&quot;mongoLewis&quot; mongo-ref=&quot;mongo&quot;/&gt; &lt;!-- 只要使用这个调用相应的方法操作 --&gt; &lt;bean id=&quot;mongoTemplate&quot; class=&quot;org.springframework.data.mongodb.core.MongoTemplate&quot;&gt; &lt;constructor-arg name=&quot;mongoDbFactory&quot; ref=&quot;mongoDbFactory&quot;/&gt; &lt;/bean&gt; &lt;!-- mongodb bean的仓库目录，会自动扫描扩展了MongoRepository接口的接口进行注入 --&gt; &lt;mongo:repositories base-package=&quot;com.lewis.mongo&quot;/&gt;&lt;/beans&gt; mongo.properties: 123456789101112mongo.hostport=10.10.16.234:27017 mongo.connectionsPerHost=8 mongo.threadsAllowedToBlockForConnectionMultiplier=4 #连接超时时间mongo.connectTimeout=1000 #等待时间mongo.maxWaitTime=1500 mongo.autoConnectRetry=true mongo.socketKeepAlive=true #Socket超时时间mongo.socketTimeout=1500 mongo.slaveOk=true pom.xml，这里要注意的是junit版本需要4.12以上，不然idea会报错；spring-data-mongodb版本要1.10.1以上； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;properties&gt; &lt;!-- spring-framework-bom 版本号--&gt; &lt;spring.version&gt;4.3.8.RELEASE&lt;/spring.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;groupId&gt;com.liu&lt;/groupId&gt; &lt;artifactId&gt;mongo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;mongo Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;!--使用junit4，注解的方式测试--&gt; &lt;!--&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;--&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--日志--&gt; &lt;!--日志 slf4j,log4j,logback,common-logging--&gt; &lt;!--slf4j是规范/接口--&gt; &lt;!--log4j,logback,common-logging是日志实现 本项目使用slf4j + logback --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt; &lt;/dependency&gt; &lt;!--实现slf4j并整合--&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--数据库相关--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.22&lt;/version&gt; &lt;!--maven工作范围 驱动在真正工作的时候使用，故生命周期改为runtime--&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!--servlet web相关 jstl1.2以上版本就不需要standard这个包了， 不然会报错： TLD skipped. URI: http://java.sun.com/jstl/core_rt is already defined--&gt; &lt;!--&lt;dependency&gt; &lt;groupId&gt;taglibs&lt;/groupId&gt; &lt;artifactId&gt;standard&lt;/artifactId&gt; &lt;version&gt;1.1.2&lt;/version&gt; &lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring--&gt; &lt;!--spring核心--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-asm --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-asm&lt;/artifactId&gt; &lt;version&gt;3.1.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring dao--&gt; &lt;!--&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-mongodb&lt;/artifactId&gt; &lt;version&gt;1.8.0.RELEASE&lt;/version&gt; &lt;/dependency&gt;--&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework.data/spring-data-mongodb --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-mongodb&lt;/artifactId&gt; &lt;version&gt;1.10.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-aop --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;4.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring web--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;4.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring test--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;4.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;/groupId&gt; &lt;artifactId&gt;commons-collections&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/javax/javaee-api --&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;version&gt;7.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-framework-bom&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache-core&lt;/artifactId&gt; &lt;version&gt;2.6.9&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;finalName&gt;mongo&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.6&lt;/source&gt; &lt;target&gt;1.6&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 两个实体类： Address.java: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.lewis.mongo.entity;/** * Created by liu on 2017/6/7. */public class Address { private String city; private String street; private int num; public Address() { } public Address(String city, String street, int num) { this.city = city; this.street = street; this.num = num; } public String getCity() { return city; } public void setCity(String city) { this.city = city; } public String getStreet() { return street; } public void setStreet(String street) { this.street = street; } public int getNum() { return num; } public void setNum(int num) { this.num = num; } @Override public String toString() { return &quot;Address{&quot; + &quot;city='&quot; + city + '\\'' + &quot;, street='&quot; + street + '\\'' + &quot;, num=&quot; + num + '}'; }} Person.java: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.lewis.mongo.entity;import java.io.Serializable;import org.bson.types.ObjectId;import org.springframework.data.annotation.Id;import org.springframework.data.mongodb.core.mapping.Document;/** * Created by liu on 2017/6/7. */@Document(collection = &quot;person&quot;)public class Person implements Serializable { @Id private ObjectId id; private String name; private int age; private Address address; public Person() { } public Person(String name, int age, Address address) { this.name = name; this.age = age; this.address = address; } public ObjectId getId() { return id; } public void setId(ObjectId id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public Address getAddress() { return address; } public void setAddress(Address address) { this.address = address; } @Override public String toString() { return &quot;Person{&quot; + &quot;id=&quot; + id + &quot;, name='&quot; + name + '\\'' + &quot;, age=&quot; + age + &quot;, address=&quot; + address + '}'; }} JPA的dao，注意这里只要继承MongoRepository不用写注解spring就能认识这是个Repository，MongoRepository提供了基本的增删改查，不用实现便可直接调用，例如testMongo的personDao.save(persons); PersonDao.java: 123456789101112131415161718package com.lewis.mongo.dao;import com.lewis.mongo.entity.Person;import org.bson.types.ObjectId;import org.springframework.data.mongodb.repository.MongoRepository;import org.springframework.data.mongodb.repository.Query;import java.util.List;/** * Created by liu on 2017/6/7. */public interface PersonDao extends MongoRepository&lt;Person, ObjectId&gt; { @Query(value = &quot;{'age' : {'$gte' : ?0, '$lte' : ?1}, 'name' : ?2}&quot;, fields = &quot;{'name' : 1, 'age' : 1}&quot;) List&lt;Person&gt; findByAge(int age1, int age2, String name);} mongoTemplate的dao，PersonMongoDao.java: 123456789101112131415package com.lewis.mongo.mongoDao;import com.lewis.mongo.entity.Person;import java.util.List;/** * Created by liu on 2017/6/7. */public interface PersonMongoDao { List&lt;Person&gt; findAll(); void insertPerson(Person user); void removePerson(String userName); void updatePerson(); List&lt;Person&gt; findForRequery(String userName);} PersonMongoImpl.java: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.lewis.mongo.mongoDao;import com.lewis.mongo.entity.Person;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.data.mongodb.core.query.Query;import org.springframework.data.mongodb.core.query.Update;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.stereotype.Repository;import javax.annotation.Resource;import java.util.List;/** * Created by liu on 2017/6/7. */@Repository(&quot;personMongoImpl&quot;)public class PersonMongoImpl implements PersonMongoDao { @Resource private MongoTemplate mongoTemplate; @Override public List&lt;Person&gt; findAll() { return mongoTemplate.findAll(Person.class, &quot;person&quot;); } @Override public void insertPerson(Person person) { mongoTemplate.insert(person, &quot;person&quot;); } @Override public void removePerson(String userName) { mongoTemplate.remove(Query.query(Criteria.where(&quot;name&quot;).is(userName)), &quot;person&quot;); } @Override public void updatePerson() { mongoTemplate.updateMulti(Query.query(Criteria.where(&quot;age&quot;).gt(3).lte(5)), Update.update(&quot;age&quot;, 3), &quot;person&quot;); } @Override public List&lt;Person&gt; findForRequery(String userName) { return mongoTemplate.find(Query.query(Criteria.where(&quot;name&quot;).is(userName)), Person.class); }} JPA查询的测试类，PersonDaoTest.java: 1234567891011121314151617181920212223242526272829303132333435363738import com.lewis.mongo.dao.PersonDao;import com.lewis.mongo.entity.Address;import com.lewis.mongo.entity.Person;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import javax.annotation.Resource;import java.util.ArrayList;import java.util.List;/** * Created by liu on 2017/6/7. */@RunWith(SpringJUnit4ClassRunner.class)//告诉junit spring配置文件@ContextConfiguration({&quot;classpath:spring/spring-context.xml&quot;, &quot;classpath:spring/spring-mongo.xml&quot;})public class PersonDaoTest { @Resource private PersonDao personDao; /*先往数据库中插入10个person*/ @Test public void testMongo() { List&lt;Person&gt; persons = new ArrayList&lt;Person&gt;(); for (int i = 0; i &lt; 10; i++) { persons.add(new Person(&quot;name&quot; + i, i, new Address(&quot;广州市&quot;, &quot;天河区&quot;, i))); } personDao.save(persons); } @Test public void findMongo() { System.out.println(personDao.findByAge(2, 8, &quot;name6&quot;)); }} mongoTemplate查询的测试类，MongoTemplateTest.java: 123456789101112131415161718192021222324252627282930import com.lewis.mongo.entity.Address;import com.lewis.mongo.entity.Person;import com.lewis.mongo.mongoDao.PersonMongoImpl;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import javax.annotation.Resource;/** * Created by liu on 2017/6/7. */@RunWith(SpringJUnit4ClassRunner.class)//告诉junit spring配置文件@ContextConfiguration({&quot;classpath:spring/spring-context.xml&quot;, &quot;classpath:spring/spring-mongo.xml&quot;})public class MongoTemplateTest { @Resource private PersonMongoImpl personMongo; @Test public void testMongoTemplate() { //personMongo.insertPerson(new Person(&quot;Lewis&quot;,24,new Address(&quot;广州&quot;,&quot;天河&quot;,20))); personMongo.removePerson(&quot;name3&quot;); personMongo.updatePerson(); System.out.println(personMongo.findAll()); System.out.println(personMongo.findForRequery(&quot;Lewis&quot;)); }} 注意测试前请先通过PersonDaoTest.java中的testMongo()方法向数据库中插入数据。 项目源码Git地址，仅供学习使用：https://github.com/wonderomg/SpringData-mongdb 参考资料：http://docs.spring.io/spring-data/mongodb/docs/current/reference/html/ 参考文章：http://www.imooc.com/article/13777","link":"/2017/06/06/idea%E6%90%AD%E5%BB%BAspringdata+mongodb+maven+springmvc/"},{"title":"Elasticsearch实践(3)-api分页查询","text":"elasticsearch系列： (1)Elasticsearch实践(1)-搭建及IK中文分词 (2)Elasticsearch实践(2)-索引及索引别名alias (3)Elasticsearch实践(3)-api分页查询 es分页有两种，from size浅分页和scroll深分页，这里对这两种分页都做了实现，使用的是es的java api。from size类似于mysql的limit分页，from偏移，默认为0，size为返回的结果数量，默认为10。在数据量不大的情况下一般会使用from size，数据量大的时候效率比较低，而且很费内存，每次会把from*size条记录全部加载到内存中，对结果返回前进行全局排序，然后丢弃掉范围外的结果，重复这样的操作会导致内存占用过大而使es挂掉，并且受数据条数限制，10000条，需修改索引限制。🤔 scroll分页通俗来说就是滚屏翻页，设置每页查询数量之后，每次查询会返回一个scroll_id，即当前文档的位置，下次查询再传这个scroll_id给es返回下一页的数据以及一个新的scroll_id，类似于按书页码顺序翻书和游标，遗憾的是不支持跳页。🤔适用于不需要跳页持续批量拉取结果、对所有数据分页或一次性查询大量数据的场景，这种记录文档位置的查询方式效率非常高，也就是常说的倒排索引。下面使用java api实现这两种分页效果。 1. 使用from size查询使用 from and size 的深度分页，比如说 ?size=10&amp;from=10000 是非常低效的，因为 100,000 排序的结果必须从每个分片上取出并重新排序最后返回 10 条。这个过程需要对每个请求页重复。 123curl -XPOST 10.10.13.234:9200/goods/fulltext/_search -H 'Content-Type:application/json' -d'{ &quot;from&quot; : 0, &quot;size&quot; : 10, &quot;query&quot; : { &quot;match&quot; : { &quot;content&quot; : &quot;进口水果&quot; }}}' 2. 使用scroll查询使用scroll查询，需要设置scroll_id的失效时间。 1234curl '10.10.13.234:9200/index/skuId/_search?scroll=1m' -d '{ &quot;query&quot; : { &quot;match&quot; : { &quot;name&quot; : &quot;进口水果&quot; }}}' 会返回scroll_id，根据这个scroll_id进行下一次查询 12345curl -XGET 10.10.13.234:9200/jd_mall_v2/_search?pretty&amp;scroll=2m -d {&quot;query&quot;:{&quot;match_all&quot;:{}}}curl -XGET '10.10.13.234:9200/jd_mall_v2/_search?pretty&amp;scroll=2m&amp;search_type=scan' -d '{&quot;size&quot;:3,&quot;query&quot;:{&quot;match_all&quot;:{}}}'curl –XGET '10.10.13.234:9200/_search/scroll?scroll=2m&amp;pretty&amp;scroll_id=c2Nhbjs1OzcyNTY6N1UtOEx3MmhSQXk2SjFnamw4bk9OUTs3MjYwOjdVLThMdzJoUkF5NkoxZ2psOG5PTlE7NzI1Nzo3VS04THcyaFJBeTZKMWdqbDhuT05ROzcyNTg6N1UtOEx3MmhSQXk2SjFnamw4bk9OUTs3MjU5OjdVLThMdzJoUkF5NkoxZ2psOG5PTlE7MTt0b3RhbF9oaXRzOjUxNTM3MDs=' 1. 使用es的java api操作java api可查阅https://es.quanke.name/， 创建springboot工程，可以在http://start.spring.io/ 下载springboot工程模板，也可在github:https://github.com/wonderomg/elasticsearch-visual下载本工程。 (1)导入es的maven依赖，pom.xml内容： 12345678&lt;!-- Elasticsearch核心依赖包 --&gt;&lt;!-- https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch --&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt; 1.1 判断索引是否存在检验索引是否创建成功，是否存在，通过prepareExists查询指定索引； 123456789101112131415/** * 1.判断索引是否存在 * * @param client * @param index * @return */ public boolean isIndexExists(Client client, String index) { IndicesAdminClient indicesAdminClient = client.admin().indices(); IndicesExistsResponse response = indicesAdminClient.prepareExists(index).get(); return response.isExists(); /* 另一种方式 IndicesExistsRequest indicesExistsRequest = new IndicesExistsRequest(index); IndicesExistsResponse response = client.admin().indices().exists(indicesExistsRequest).actionGet();*/ } 1.2 判断类型是否存在类型存在的检查与索引的检查类似，只不过类型只挂在索引下，所以需要先指定索引名，然后再查询类型是否存在； 12345678910111213141516/** * 2.判断类型是否存在 * * @param client * @param index * @param type * @return */ public boolean isTypeExists(Client client, String index, String type) { if (!isIndexExists(client, index)) { return false; } IndicesAdminClient indicesAdminClient = client.admin().indices(); TypesExistsResponse response = indicesAdminClient.prepareTypesExists(index).setTypes(type).get(); return response.isExists(); } 1.3 创建复杂索引12345678910111213141516171819202122232425262728293031323334353637383940/** * 3.创建复杂索引，即有映射 * * @param indices * @param type * @param clusterName * @param host * @param port * @return */ public boolean createYxMapping(String indices, String type, String clusterName, String host, int port) { try { Settings settings = Settings.settingsBuilder() .put(&quot;cluster.name&quot;, clusterName) .put(&quot;client.transport.sniff&quot;, true) .build(); Client client = TransportClient.builder().settings(settings).build() .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(host), port)); client.admin().indices().prepareCreate(indices).execute().actionGet(); new XContentFactory(); XContentBuilder builder = XContentFactory.jsonBuilder() .startObject() .startObject(type) .startObject(&quot;properties&quot;) .startObject(&quot;name&quot;).field(&quot;type&quot;, &quot;string&quot;).field(&quot;store&quot;, &quot;yes&quot;).field(&quot;analyzer&quot;, &quot;ik&quot;).field(&quot;index&quot;, &quot;analyzed&quot;).endObject() .startObject(&quot;num&quot;).field(&quot;type&quot;, &quot;long&quot;).endObject() .startObject(&quot;sex&quot;).field(&quot;type&quot;, &quot;string&quot;).field(&quot;index&quot;, &quot;not_analyzed&quot;).endObject() .endObject() .endObject() .endObject(); PutMappingRequest mapping = Requests.putMappingRequest(indices).type(type).source(builder); client.admin().indices().putMapping(mapping).actionGet(); client.close(); return true; } catch (Exception e) { logger.error(&quot;createMapping error &quot;, e); } return false; } 1.4 创建空索引创建一个空索引，无映射mapping，当我们设计好mapping的相关设置再映射即可，空索引创建如下： 1234567891011/** * 4.创建空索引 默认setting 无mapping * @param client * @param index * @return */public boolean createSimpleIndex(Client client, String index){ IndicesAdminClient indicesAdminClient = client.admin().indices(); CreateIndexResponse response = indicesAdminClient.prepareCreate(index).get(); return response.isAcknowledged();} 1.5 为空索引映射mapping可以为空索引创建或修改索引的映射，需要确保索引存在，否则会报错，使用如下： 1234567891011121314151617181920212223242526272829/** * 5.设置映射 * @param client * @param index * @param type * @return */public boolean putIndexMapping(Client client, String index, String type){ // mapping XContentBuilder mappingBuilder; try { mappingBuilder = XContentFactory.jsonBuilder() .startObject() .startObject(type) .startObject(&quot;properties&quot;) .startObject(&quot;name&quot;).field(&quot;type&quot;, &quot;string&quot;).field(&quot;store&quot;, &quot;yes&quot;).field(&quot;analyzer&quot;, &quot;ik&quot;).field(&quot;index&quot;, &quot;analyzed&quot;).endObject() .startObject(&quot;num&quot;).field(&quot;type&quot;, &quot;long&quot;).endObject() .startObject(&quot;sex&quot;).field(&quot;type&quot;, &quot;string&quot;).field(&quot;index&quot;, &quot;not_analyzed&quot;).endObject() .endObject() .endObject() .endObject(); } catch (Exception e) { logger.error(&quot;--------- createIndex 创建 mapping 失败：&quot;, e); return false; } IndicesAdminClient indicesAdminClient = client.admin().indices(); PutMappingResponse response = indicesAdminClient.preparePutMapping(index).setType(type).setSource(mappingBuilder).get(); return response.isAcknowledged();} 1.6 删除索引删除索引api： 1234567891011/** * 6.删除索引 * * @param client * @param index */ public boolean deleteIndex(Client client, String index) { IndicesAdminClient indicesAdminClient = client.admin().indices(); DeleteIndexResponse response = indicesAdminClient.prepareDelete(index).execute().actionGet(); return response.isAcknowledged(); } 1.7 关闭索引如果我们不想直接删除索引，而是仅仅短暂停止某个索引的使用，那么我们可以关闭索引的这个功能，只是短暂使用之，后续可能有恢复的需求，关闭使用如下： 123456789101112/** * 7.关闭索引 * @param client * @param index * @return */public boolean closeIndex(Client client, String index){ IndicesAdminClient indicesAdminClient = client.admin().indices(); CloseIndexResponse response = indicesAdminClient.prepareClose(index).get(); return response.isAcknowledged();} 1.8 打开索引既然有关闭索引，那么肯定也有重新打开索引的操作，使用如下： 1234567891011/** * 8.开启索引 * @param client * @param index * @return */public boolean openIndex(Client client, String index){ IndicesAdminClient indicesAdminClient = client.admin().indices(); OpenIndexResponse response = indicesAdminClient.prepareOpen(index).get(); return response.isAcknowledged();} 1.9 判断别名是否存在上一篇我们学习了索引别名的重要，所以api也不能落下，使用如下： 1234567891011/** * 9.判断别名是否存在 * * @param * @return */ public boolean isAliasExist(Client client, String... aliases) { IndicesAdminClient indicesAdminClient = client.admin().indices(); AliasesExistResponse response = indicesAdminClient.prepareAliasesExist(aliases).get(); return response.isExists(); } 1.10 添加别名别名是映射到实际索引上，所以需要传入实际索引名以及要设置的别名alias，使用如下： 1234567891011/** * 10.添加别名 * * @param * @return */ public boolean addAlias(TransportClient client, String index, String alias) { IndicesAdminClient indicesAdminClient = client.admin().indices(); IndicesAliasesResponse response = indicesAdminClient.prepareAliases().addAlias(index, alias).get(); return response.isAcknowledged(); } 1.11 移除别名当然，添加的别名也可以删除，方便设置新的别名和切换索引别名，使用如下： 1234567891011/** * 11.移除别名 * * @param * @return */ public boolean removeAlias(TransportClient client, String index, String alias) { IndicesAdminClient indicesAdminClient = client.admin().indices(); IndicesAliasesResponse response = indicesAdminClient.prepareAliases().removeAlias(index, alias).get(); return response.isAcknowledged(); } 1.12 删除一个别名后再添加一个别名这里实际上将删除与添加别名放在一起操作，所以需要传就别名和新别名，使用如下： 12345678910111213/** * 12.删除一个别名后再添加一个 * * @param * @return */ @Override public boolean removeAndCreateAlias(TransportClient client, String indexOld, String indexNew, String alias) { IndicesAdminClient indicesAdminClient = client.admin().indices(); IndicesAliasesResponse response = indicesAdminClient.prepareAliases().removeAlias(indexOld, alias) .addAlias(indexNew, alias).get(); return response.isAcknowledged(); } 2. 创建测试基类连接es创建test类测试连接、插入、查询数据是否正常。 这里可以模拟一批数据插入es，后面的分页操作对大量数据才看得出效果，我这里有的40万数据加入到es中，方便后面的分页测试。 ElasticsearchTest.java: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611import com.alibaba.fastjson.JSON;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import com.sun.controller.DateUtils;import org.elasticsearch.action.admin.indices.analyze.AnalyzeAction;import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder;import org.elasticsearch.action.admin.indices.analyze.AnalyzeResponse;import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;import org.elasticsearch.action.delete.DeleteResponse;import org.elasticsearch.action.get.GetResponse;import org.elasticsearch.action.index.IndexResponse;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.action.search.SearchType;import org.elasticsearch.action.update.UpdateResponse;import org.elasticsearch.client.Client;import org.elasticsearch.client.Requests;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.InetSocketTransportAddress;import org.elasticsearch.common.unit.TimeValue;import org.elasticsearch.common.xcontent.XContentBuilder;import org.elasticsearch.common.xcontent.XContentFactory;import org.elasticsearch.index.query.*;import org.elasticsearch.search.SearchHits;import org.junit.Before;import org.junit.Test;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.BufferedReader;import java.io.File;import java.io.FileInputStream;import java.io.InputStreamReader;import java.net.InetAddress;import java.net.UnknownHostException;import java.util.ArrayList;import java.util.List;import java.util.Map;import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;/** * 类功能描述:Elasticsearch的基本测试 * * @author liu * @since 2018/5/24 */public class ElasticsearchTest { private Logger logger = LoggerFactory.getLogger(ElasticsearchTest.class); public final static String HOST = &quot;10.10.13.234&quot;; /** http请求的端口是9200，客户端是9300 */ public final static int PORT = 9300; Client client; /** 索引库名 */ private static String index = &quot;my_index&quot;; /** 类型名称 */ private static String type = &quot;skuId&quot;; private static String clusterName = &quot;my-application&quot;; /** * 创建mapping，注意：每次只能创建一次 * 创建mapping(field(&quot;indexAnalyzer&quot;,&quot;ik&quot;)该字段分词IK索引 ；field(&quot;searchAnalyzer&quot;,&quot;ik&quot;)该字段分词ik查询；具体分词插件请看IK分词插件说明) * 创建mapping(field(&quot;analyzer&quot;,&quot;ik&quot;)该字段分词IK索引 ；field(&quot;index&quot;,&quot;analyzed&quot;)该字段分词ik查询；具体分词插件请看IK分词插件说明)2.x版 * @param indices 索引名称； * @param mappingType 类型 * @throws Exception */ @Test public void createMapping()throws Exception { String indices = &quot;jd_mall_v2&quot;; String mappingType = &quot;goods&quot;; Settings settings = Settings.settingsBuilder() .put(&quot;cluster.name&quot;, clusterName) .put(&quot;client.transport.sniff&quot;, true) .build(); Client client = TransportClient.builder().settings(settings).build() .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(HOST), PORT)); client.admin().indices().prepareCreate(indices).execute().actionGet(); new XContentFactory(); XContentBuilder builder=XContentFactory.jsonBuilder() .startObject() .startObject(mappingType) .startObject(&quot;properties&quot;) .startObject(&quot;name&quot;).field(&quot;type&quot;, &quot;string&quot;).field(&quot;store&quot;, &quot;yes&quot;).field(&quot;analyzer&quot;,&quot;ik&quot;).field(&quot;index&quot;,&quot;analyzed&quot;).endObject() .startObject(&quot;price&quot;).field(&quot;type&quot;, &quot;long&quot;).endObject() .endObject() .endObject() .endObject(); PutMappingRequest mapping = Requests.putMappingRequest(indices).type(mappingType).source(builder); client.admin().indices().putMapping(mapping).actionGet(); client.close(); } @Before public void before() { /** * 1:通过 setting对象来指定集群配置信息 * //指定集群名称 * //启动嗅探功能 */ Settings settings = Settings.settingsBuilder() .put(&quot;cluster.name&quot;, clusterName) .put(&quot;client.transport.sniff&quot;, true) .build(); /** * 2：创建客户端 * 通过setting来创建，若不指定则默认链接的集群名为elasticsearch * 链接使用tcp协议即9300 */ try { client = TransportClient.builder().settings(settings).build() .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(HOST), PORT)); System.out.println(&quot;Elasticsearch connect info:&quot; + client.toString()); } catch (UnknownHostException e) { e.printStackTrace(); } /*transportClient = new TransportClient(setting); TransportAddress transportAddress = new InetSocketTransportAddress(&quot;192.168.79.131&quot;, 9300); transportClient.addTransportAddresses(transportAddress);*/ /** * 3：查看集群信息 * 注意我的集群结构是： * 131的elasticsearch.yml中指定为主节点不能存储数据， * 128的elasticsearch.yml中指定不为主节点只能存储数据。 * 所有控制台只打印了192.168.79.128,只能获取数据节点 * */ /*ImmutableList&lt;DiscoveryNode&gt; connectedNodes = client.connectedNodes(); for(DiscoveryNode node : connectedNodes) { System.out.println(node.getHostAddress()); }*/ } /** * 测试Elasticsearch客户端连接 * @Title: test1 * @author sunt * @date 2017年11月22日 * @return void * @throws UnknownHostException */ @SuppressWarnings(&quot;resource&quot;) @Test public void test1() throws UnknownHostException { //创建客户端 /*TransportClient client = new PreBuiltTransportClient(Settings.EMPTY).addTransportAddresses( new InetSocketTransportAddress(InetAddress.getByName(HOST),PORT));*/ TransportClient client = TransportClient.builder().build().addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(HOST), PORT)); logger.debug(&quot;Elasticsearch connect info:&quot; + client.toString()); System.out.println(&quot;Elasticsearch connect info:&quot; + client.toString()); //关闭客户端 client.close(); } /** * 查 * * @throws Exception */ @Test public void testGet() throws Exception { String skuId = &quot;3724941&quot;; String skuName = &quot;华盛顿苹果礼盒 8个装（4个红蛇果+4个青苹果）单果重约145g-180g 新鲜水果礼盒&quot;; String skuPrice = &quot;4990&quot;; //获取_id为1的类型 /*GetResponse response1 = client.prepareGet(index, &quot;person&quot;, &quot;9&quot;).get(); response1.getSourceAsMap();//获取值转换成map System.out.println(&quot;查询一条数据:&quot; + JSON.toJSON(response1.getSourceAsMap()));*/ //获取分词 List&lt;String&gt; listAnalysis = getIkAnalyzeSearchTerms(&quot;进口水果&quot;); //List&lt;Map&lt;String, Object&gt;&gt; resultList6 = testMultiQueryStringQuery(index, type, &quot;name&quot;, listAnalysis); List&lt;Map&lt;String, Object&gt;&gt; resultList3 = testQueryStringQuery(index, type, &quot;name&quot;, &quot;进口水果&quot;); //List&lt;Map&lt;String, Object&gt;&gt; resultList4 = getDataByWildcard(index, type, &quot;name&quot;, &quot;*水果*&quot;); //List&lt;Map&lt;String, Object&gt;&gt; resultList5 = getDataByMatch(index, type, &quot;name&quot;, &quot;进口 水果&quot;); //List&lt;Map&lt;String, Object&gt;&gt; resultList = getQueryDataBySingleField(index, type, &quot;name&quot;, &quot;进口水果&quot;); //List&lt;Map&lt;String, Object&gt;&gt; resultList2 = testSearchByPrefix(index, &quot;name&quot;, &quot;进口水果&quot;); //System.out.println(); } /** * 增查 * * @throws Exception */ @Test public void testCreate() throws Exception { String[] skuIds = {&quot;2246904&quot;, &quot;2138027&quot;, &quot;3724941&quot;, &quot;5664705&quot;, &quot;3711635&quot;}; String[] skuNames = {&quot;果花 珍珠岩 无土栽培基质 颗粒状 保温性能好 园艺用品&quot;, &quot;进口华盛顿红蛇果 苹果4个装 单果重约180g 新鲜水果&quot;, &quot;华盛顿苹果礼盒 8个装（4个红蛇果+4个青苹果）单果重约145g-180g 新鲜水果礼盒&quot;, &quot;新疆阿克苏冰糖心 约5kg 单果200-250g（7Fresh 专供）&quot;, &quot;江西名牌 杨氏精品脐橙 5kg装 铂金果 水果橙子礼盒 2种包装随机发货&quot;}; String[] skuPrices = {&quot;500&quot;, &quot;3800&quot;, &quot;4990&quot;, &quot;8500&quot;, &quot;5880&quot;}; for (int i=0; i&lt;skuIds.length; i++) { String skuId = skuIds[i]; String skuName = skuNames[i]; String skuPrice = skuPrices[i]; String jsonStr = &quot;{&quot; + &quot;\\&quot;skuId\\&quot;:\\&quot;&quot;+skuId+&quot;\\&quot;,&quot; + &quot;\\&quot;skuName\\&quot;:\\&quot;&quot;+skuName+&quot;\\&quot;,&quot; + &quot;\\&quot;skuPrice\\&quot;:\\&quot;&quot;+skuPrice+&quot;\\&quot;&quot; + &quot;}&quot;; //参数说明： 索引，类型 ，_id；//setSource可以传以上map string byte[] 几种方式 IndexResponse response = client.prepareIndex(index, type, skuId) .setSource(jsonStr,XContentType.JSON) .get(); boolean created = response.isCreated(); System.out.println(&quot;创建一条记录:&quot; + created); //获取_id为1的类型 GetResponse response1 = client.prepareGet(index, type, skuId).get(); response1.getSourceAsMap();//获取值转换成map System.out.println(&quot;查询一条数据:&quot; + JSON.toJSON(response1.getSourceAsMap())); } } /** * 删查 * * @throws Exception */ @Test public void testDelete() throws Exception { String skuId = &quot;3724941&quot;; String skuName = &quot;华盛顿苹果礼盒 8个装（4个红蛇果+4个青苹果）单果重约145g-180g 新鲜水果礼盒&quot;; String skuPrice = &quot;4990&quot;; String skuImage = &quot;jfs/t16450/249/461724533/219792/7f204d7a/5a321ecbN4526f7d3.jpg&quot;; //删除_id为1的类型 DeleteResponse response2 = client.prepareDelete(index, type, skuId).get(); System.out.println(&quot;删除一条数据：&quot; + response2.isFound()); //获取_id为1的类型 GetResponse response1 = client.prepareGet(index, type, skuId).get(); response1.getSourceAsMap();//获取值转换成map System.out.println(&quot;查询一条数据:&quot; + JSON.toJSON(response1.getSourceAsMap())); } /** * 改查 * * @throws Exception */ @Test public void testUpdate() throws Exception { String skuId = &quot;3724941&quot;; String skuName = &quot;华盛顿苹果礼盒 8个装（4个红蛇果+4个青苹果）单果重约145g-180g 新鲜水果礼盒&quot;; String skuPrice = &quot;4990&quot;; String skuName2 = &quot;华圣 陕西精品红富士苹果 12个装 果径75mm 约2.1kg 新鲜水果&quot;; //更新 UpdateResponse updateResponse = client.prepareUpdate(index, type, skuId).setDoc(jsonBuilder() .startObject() .field(&quot;name&quot;, skuName2) .endObject()) .get(); System.out.println(&quot;更新一条数据:&quot; + updateResponse.isCreated()); //获取_id为1的类型 GetResponse response1 = client.prepareGet(index, type, skuId).get(); response1.getSourceAsMap();//获取值转换成map System.out.println(&quot;查询一条数据:&quot; + JSON.toJSON(response1.getSourceAsMap())); } /** * 将对象通过jackson.databind转换成byte[] * 注意一下date类型需要格式化处理 默认是 时间戳 * * @return */ public byte[] convertByteArray(Object obj) { // create once, reuse ObjectMapper mapper = new ObjectMapper(); try { byte[] json = mapper.writeValueAsBytes(obj); return json; } catch (JsonProcessingException e) { e.printStackTrace(); } return null; } /** * 将对象通过JSONtoString转换成JSON字符串 * 使用fastjson 格式化注解 在属性上加入 @JSONField(format=&quot;yyyy-MM-dd HH:mm:ss&quot;) * * @return */ public String jsonStr(Object obj) { return JSON.toJSONString(obj); } /*****************************/ /** * 根据文档名、字段名、字段值查询某一条记录的详细信息；query查询 * @param type 文档名，相当于oracle中的表名，例如：ql_xz； * @param key 字段名，例如：bdcqzh * @param value 字段值，如：“” * @return List * @author Lixin */ public List getQueryDataBySingleField(String index, String type, String key, String value){ //TransportClient client = getClient(); QueryBuilder qb = QueryBuilders.termQuery(key, value); SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(qb) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } /** * 多条件 文档名、字段名、字段值，查询某一条记录的详细信息 * @param type 文档名，相当于oracle中的表名，例如：ql_xz * @param map 字段名：字段值 的map * @return List * @author Lixin */ public List getBoolDataByMuchField(String index, String type, Map&lt;String,String&gt; map){ //TransportClient client = getClient(); BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); for (String in : map.keySet()) { //map.keySet()返回的是所有key的值 String str = map.get(in);//得到每个key多对用value的值 boolQueryBuilder.must(QueryBuilders.termQuery(in,str)); } SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } /** * 单条件 通配符查询 * @param type 文档名，相当于oracle中的表名，例如：ql_xz * @param key 字段名，例如：bdcqzh * @param value 字段名模糊值：如 *123* ;?123*;?123?;*123?; * @return List * @author Lixin */ public List getDataByWildcard(String index, String type, String key, String value){ //TransportClient client = getClient(); WildcardQueryBuilder wBuilder = QueryBuilders.wildcardQuery(key, value); SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(wBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } /** * 多条件 通配符查询 * @param type type 文档名，相当于oracle中的表名，例如：ql_xz * @param map 包含key:value 模糊值键值对 * @return List * @author Lixin */ public List getDataByMuchWildcard(String index, String type, Map&lt;String,String&gt; map){ //TransportClient client = getClient(); BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); for (String in : map.keySet()) { //map.keySet()返回的是所有key的值 String str = map.get(in);//得到每个key多对用value的值 boolQueryBuilder.must(QueryBuilders.wildcardQuery(in,str)); } SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } /** * 单条件 模糊拆分查询 * @param type 文档名，相当于oracle中的表名，例如：ql_xz * @param key 字段名，例如：bdcqzh * @param value 字段名模糊值：如 *123* ;?123*;?123?;*123?; * @return List * @author Lixin */ public List getDataByMatch(String index, String type, String key, String value){ //TransportClient client = getClient(); MatchQueryBuilder mBuilder = QueryBuilders.matchQuery(key, value); SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(mBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } /** * 单条件 中文分词模糊查询 * @param type 文档名，相当于oracle中的表名，例如：ql_xz * @param key 字段名，例如：bdcqzh * @param value 字段名模糊值：如 *123* ;?123*;?123?;*123?; * @return List * @author Lixin */ public List getDataByPrefix(String index, String type, String key, String value){ //TransportClient client = getClient(); PrefixQueryBuilder pBuilder = QueryBuilders.prefixQuery(key, value); SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(pBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } /** * 中文分词的操作 * 1.查询以&quot;中&quot;开头的数据，有两条 * 2.查询以“中国”开头的数据，有0条 * 3.查询包含“烂”的数据，有1条 * 4.查询包含“烂摊子”的数据，有0条 * 分词： * 为什么我们搜索China is the greatest country~ * 中文：中国最牛逼 * * ××× * 中华 * 人民 * 共和国 * 中华人民 * 人民共和国 * 华人 * 共和 * 特殊的中文分词法： * 庖丁解牛 * IK分词法 * 搜狗分词法 */ public List testSearchByPrefix(String index, String key, String value) { // 在prepareSearch()的参数为索引库列表，意为要从哪些索引库中进行查询 SearchResponse response = client.prepareSearch(index) .setSearchType(SearchType.DEFAULT) // 设置查询类型，有QUERY_AND_FETCH QUERY_THEN_FETCH DFS_QUERY_AND_FETCH DFS_QUERY_THEN_FETCH //.setSearchType(SearchType.DEFAULT) // 设置查询类型，有QUERY_AND_FETCH QUERY_THEN_FETCH DFS_QUERY_AND_FETCH DFS_QUERY_THEN_FETCH //.setQuery(QueryBuilders.prefixQuery(&quot;content&quot;, &quot;烂摊子&quot;))// 设置相应的query，用于检索，termQuery的参数说明：name是doc中的具体的field，value就是要找的具体的值// .setQuery(QueryBuilders.regexpQuery(&quot;content&quot;, &quot;.*烂摊子.*&quot;)) .setQuery(QueryBuilders.prefixQuery(key, value)) .get(); return responseToList(client,response); } public List testFuzzyQuery(String index, String type, String key, String value){ //TransportClient client = getClient(); FuzzyQueryBuilder fBuilder = QueryBuilders.fuzzyQuery(key, value); SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(fBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } public List testQueryStringQuery(String index, String type, String key, String value){ //TransportClient client = getClient(); QueryBuilder fBuilder = QueryBuilders.queryStringQuery(value).field(key); SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(fBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } public List testMultiQueryStringQuery(String index, String type, String key, List&lt;String&gt; listAnalysis){ //TransportClient client = getClient(); BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); for (String term : listAnalysis) { boolQueryBuilder.should(QueryBuilders.queryStringQuery(term).field(key)); //这里可以用must 或者 should 视情况而定 } SearchResponse response = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setFrom(0).setSize(10000).setExplain(true) .execute() .actionGet(); return responseToList(client,response); } /** * 将查询后获得的response转成list * @param client * @param response * @return */ public List responseToList(Client client, SearchResponse response){ SearchHits hits = response.getHits(); List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;(); System.out.println(&quot;------------------------&quot;); for (int i = 0; i &lt; hits.getHits().length; i++) { Map&lt;String, Object&gt; map = hits.getAt(i).getSource(); System.out.println(map.get(&quot;name&quot;)); list.add(map); } System.out.println(&quot;------------------------&quot;); //client.close(); return list; } /** * 调用 ES 获取 IK 分词后结果 * * @param searchContent * @return */ private List&lt;String&gt; getIkAnalyzeSearchTerms(String searchContent) { //调用 IK 分词分词 AnalyzeRequestBuilder ikRequest = new AnalyzeRequestBuilder(client, AnalyzeAction.INSTANCE, index, searchContent); //ikRequest.setAnalyzer(&quot;ik&quot;); ikRequest.setTokenizer(&quot;ik&quot;); List&lt;AnalyzeResponse.AnalyzeToken&gt; ikTokenList = ikRequest.execute().actionGet().getTokens(); //循环赋值 List&lt;String&gt; searchTermList = new ArrayList&lt;&gt;(); //ikTokenList.forEach(ikToken -&gt; { searchTermList.add(ikToken.getTerm()); }); for (AnalyzeResponse.AnalyzeToken ikToken: ikTokenList) { System.out.println(ikToken.getTerm()); searchTermList.add(ikToken.getTerm()); } return searchTermList; } /** * 然后就是创建两个查询过程了 ，下面是from-size分页的执行代码： */ @Test public void testFromSize() { System.out.println(&quot;from size 模式启动！&quot;); long begin = System.currentTimeMillis(); long count = client.prepareCount(index).setTypes(type).execute() .actionGet().getCount(); //QueryBuilder qBuilder = QueryBuilders.queryStringQuery(value).field(key); SearchRequestBuilder requestBuilder = client.prepareSearch(index).setTypes(type) .setQuery(QueryBuilders.queryStringQuery(&quot;进口水果&quot;).field(&quot;name&quot;)); for (int i=0,sum=0; sum&lt;count; i++) { SearchResponse response = requestBuilder.setFrom(i).setSize(3).execute().actionGet(); sum += response.getHits().hits().length; responseToList(client,response); System.out.println(&quot;总量&quot;+count+&quot;, 已经查到&quot;+sum); } System.out.println(&quot;elapsed&lt;&quot;+(System.currentTimeMillis()-begin)+&quot;&gt;ms.&quot;); } /** * 下面是scroll分页的执行代码，注意啊！scroll里面的size是相对于每个分片来说的， * 所以实际返回的数量是：分片的数量*size */ @Test public void testScroll() { //String index, String type,String key,String value System.out.println(&quot;scroll 模式启动！&quot;); long begin = System.currentTimeMillis(); //QueryBuilder qBuilder = QueryBuilders.queryStringQuery(value).field(key); //获取Client对象,设置索引名称,搜索类型(SearchType.SCAN),搜索数量,发送请求 SearchResponse scrollResponse = client.prepareSearch(index) .setTypes(type) .setQuery(QueryBuilders.queryStringQuery(&quot;进口水果&quot;).field(&quot;name&quot;)) .setSearchType(SearchType.SCAN).setSize(3).setScroll(TimeValue.timeValueMinutes(1)) .execute().actionGet(); //注意:首次搜索并不包含数据，第一次不返回数据，获取总数量 long count = scrollResponse.getHits().getTotalHits(); for (int i=0,sum=0; sum&lt;count; i++) { scrollResponse = client.prepareSearchScroll(scrollResponse.getScrollId()) .setScroll(TimeValue.timeValueMinutes(8)).execute().actionGet(); sum += scrollResponse.getHits().hits().length; responseToList(client,scrollResponse); System.out.println(&quot;总量&quot;+count+&quot;, 已经查到&quot;+sum); } System.out.println(&quot;elapsed&lt;&quot;+(System.currentTimeMillis()-begin)+&quot;&gt;ms.&quot;); }} 3. 分页代码创建TestController类，在上述测试都无问题之后，再进行下面的分页操作。 TestController.java： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387package com.sun.controller;import com.alibaba.fastjson.JSONObject;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.action.search.SearchScrollRequestBuilder;import org.elasticsearch.search.SearchHit;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.elasticsearch.action.admin.indices.analyze.AnalyzeAction;import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder;import org.elasticsearch.action.admin.indices.analyze.AnalyzeResponse;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.action.search.SearchType;import org.elasticsearch.client.Client;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.InetSocketTransportAddress;import org.elasticsearch.common.unit.TimeValue;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHits;import org.elasticsearch.search.sort.SortOrder;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.bind.annotation.RestController;import java.net.InetAddress;import java.net.UnknownHostException;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.concurrent.atomic.AtomicLong;/** * 类功能描述: * * @author liu * @since 2018/2/22 */@RestControllerpublic class TestController { private Logger logger = LoggerFactory.getLogger(this.getClass()); public final static String HOST = &quot;10.10.13.234&quot;; /** http请求的端口是9200，客户端是9300 */ public final static int PORT = 9300; /** 索引库名 */ private static String index = &quot;my_index&quot;; /** 类型名称 */ private static String type = &quot;skuId&quot;; private static String clusterName = &quot;my-application&quot;; private static final String template = &quot;Hello, %s!&quot;; private final AtomicLong counter = new AtomicLong(); @ResponseBody @RequestMapping(&quot;/greeting&quot;) public Greeting greeting(@RequestParam(value=&quot;name&quot;, defaultValue=&quot;World&quot;) String name) { return new Greeting(counter.incrementAndGet(), String.format(template, name)); } public long getTotalRows(String keyword, TransportClient client) { long totalRows; BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); boolQueryBuilder.must(QueryBuilders.termQuery(&quot;isShow&quot;, 1)) .must(QueryBuilders.termQuery(&quot;status&quot;, 1)) .must(QueryBuilders.termQuery(&quot;deleteStatus&quot;, 1)) .must(QueryBuilders.termQuery(&quot;salesStatus&quot;, 1)); if (keyword == null || keyword.isEmpty()) { totalRows = client.prepareCount(index).setTypes(type) .execute() .actionGet().getCount(); } else { QueryBuilder fBuilder = QueryBuilders.queryStringQuery(keyword).field(&quot;name&quot;); boolQueryBuilder.must(fBuilder); totalRows = client.prepareCount(index).setTypes(type) .setQuery(boolQueryBuilder) .execute() .actionGet().getCount(); } return totalRows; } @ResponseBody @RequestMapping(&quot;/searchFromSize&quot;) public Object testSearchByFromSize(@RequestParam(required = false) Integer pageIndex, @RequestParam(required = false) Integer pageSize, @RequestParam(required = false) String keyword, @RequestParam(required = false) String priceSort){ logger.info(&quot;op&lt;testSearchByFromSize&gt; pageIndex&lt;&quot;+pageIndex+&quot;&gt; pageSize&lt;&quot;+pageSize+&quot;&gt; &quot; + &quot;keyword&lt;&quot;+keyword+&quot;&gt; priceSort&lt;&quot;+priceSort+&quot;&gt;&quot;); JSONObject result = new JSONObject(); JSONObject goodsResult = new JSONObject(); long begin = System.currentTimeMillis(); TransportClient client = getClient(); long totalRows = getTotalRows(keyword, client); getIkAnalyzeSearchTerms(client, keyword); int from = (pageIndex - 1) * pageSize; List&lt;Map&lt;String, Object&gt;&gt; goodsExportList = testQueryStringQueryFromSize(client, index, type, &quot;name&quot;, keyword, from, pageSize, priceSort); goodsResult.put( &quot;goods&quot;, goodsExportList ); goodsResult.put( &quot;pageIndex&quot;, pageIndex ); goodsResult.put( &quot;pageSize&quot;, pageSize ); goodsResult.put( &quot;totalRows&quot;, totalRows ); result.put( &quot;success&quot;, true ); result.put( &quot;resultMessage&quot;, &quot;&quot; ); result.put( &quot;resultCode&quot;, &quot;0000&quot; ); result.put( &quot;result&quot;, goodsResult ); System.out.println(&quot;---op&lt;searchFromSize&gt; elapsed&lt;&quot;+(System.currentTimeMillis()-begin)+&quot;&gt;ms&quot;); return result; } public String scrollIdTemp = &quot;&quot;; @ResponseBody @RequestMapping(&quot;/searchScroll&quot;) public Object testSearchByScroll(@RequestParam(required = false) String scrollId, @RequestParam(required = false) Integer pageSize, @RequestParam(required = false) String keyword, @RequestParam(required = false) String priceSort){ logger.info(&quot;op&lt;testSearchByScroll&gt; scrollId&lt;&quot;+scrollId+&quot;&gt; pageSize&lt;&quot;+pageSize+&quot;&gt; &quot; + &quot;keyword&lt;&quot;+keyword+&quot;&gt; priceSort&lt;&quot;+priceSort+&quot;&gt;&quot;); JSONObject result = new JSONObject(); JSONObject goodsResult = new JSONObject(); long begin = System.currentTimeMillis(); scrollIdTemp = scrollId; TransportClient client = getClient(); long totalRows = getTotalRows(keyword, client); getIkAnalyzeSearchTerms(client, keyword); List&lt;Map&lt;String, Object&gt;&gt; goodsExportList = testQueryStringQueryScroll(client, index, type, &quot;name&quot;, keyword, scrollIdTemp, pageSize, priceSort); client.close(); goodsResult.put( &quot;goods&quot;, goodsExportList ); goodsResult.put( &quot;scrollId&quot;, scrollIdTemp ); goodsResult.put( &quot;pageSize&quot;, pageSize ); goodsResult.put( &quot;totalRows&quot;, totalRows ); result.put( &quot;success&quot;, true ); result.put( &quot;resultMessage&quot;, &quot;&quot; ); result.put( &quot;resultCode&quot;, &quot;0000&quot; ); result.put( &quot;result&quot;, goodsResult ); System.out.println(&quot;---op&lt;searchScroll&gt; elapsed&lt;&quot;+(System.currentTimeMillis()-begin)+&quot;&gt;ms&quot;); return result; } /** * 然后就是创建两个查询过程了 ，下面是from-size分页的执行代码： */ public List testQueryStringQueryFromSize(TransportClient client, String index, String type, String key, String value, Integer pageIndex, Integer pageSize ,String priceSort){ SearchResponse response = null; List resultList = null; try { //TransportClient client = getClient(); BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); if (value == null || value.isEmpty()) { boolQueryBuilder.must(QueryBuilders.termQuery(&quot;isShow&quot;, 1)) .must(QueryBuilders.termQuery(&quot;status&quot;, 1)) .must(QueryBuilders.termQuery(&quot;deleteStatus&quot;, 1)) .must(QueryBuilders.termQuery(&quot;salesStatus&quot;, 1)); } else { QueryBuilder fBuilder = QueryBuilders.queryStringQuery(value).field(key); boolQueryBuilder.must(QueryBuilders.termQuery(&quot;isShow&quot;, 1)) .must(QueryBuilders.termQuery(&quot;status&quot;, 1)) .must(QueryBuilders.termQuery(&quot;deleteStatus&quot;, 1)) .must(QueryBuilders.termQuery(&quot;salesStatus&quot;, 1)) .must(fBuilder); } if (priceSort == null || priceSort.isEmpty()) { response = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setFrom(pageIndex).setSize(pageSize).setExplain(true) .execute() .actionGet(); } else { if ((&quot;desc&quot;).equals(priceSort)) { response = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setFrom(pageIndex).setSize(pageSize).setExplain(true) .addSort(&quot;price&quot;, SortOrder.DESC) .execute() .actionGet(); } else { response = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setFrom(pageIndex).setSize(pageSize).setExplain(true) .addSort(&quot;price&quot;, SortOrder.ASC) .execute() .actionGet(); } } resultList = responseToList(response); } catch (Exception e) { e.printStackTrace(); } return resultList; } /** * 下面是scroll分页的执行代码，注意啊！scroll里面的size是相对于每个分片来说的， * 所以实际返回的数量是：分片的数量*size * * Scroll-Scan 方式与普通 scroll 有几点不同： * 1.Scroll-Scan 结果没有排序，按 index 顺序返回，没有排序，可以提高取数据性能。 * 2.初始化时只返回 _scroll_id，没有具体的 hits 结果。 * 3.size 控制的是每个分片的返回的数据量而不是整个请求返回的数据量。 */ public List testQueryStringQueryScroll(TransportClient client, String index, String type, String key, String value, String scrollId, Integer pageSize ,String priceSort){ if (pageSize != null) { //默认有5个分片， pageSize = pageSize/5; } SearchResponse scrollResponse = null; List resultList = null; try { //TransportClient client = getClient(); BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); if (value == null || value.isEmpty()) { boolQueryBuilder.must(QueryBuilders.termQuery(&quot;isShow&quot;, 1)) .must(QueryBuilders.termQuery(&quot;status&quot;, 1)) .must(QueryBuilders.termQuery(&quot;deleteStatus&quot;, 1)) .must(QueryBuilders.termQuery(&quot;salesStatus&quot;, 1)); } else { QueryBuilder fBuilder = QueryBuilders.queryStringQuery(value).field(key); boolQueryBuilder.must(QueryBuilders.termQuery(&quot;isShow&quot;, 1)) .must(QueryBuilders.termQuery(&quot;status&quot;, 1)) .must(QueryBuilders.termQuery(&quot;deleteStatus&quot;, 1)) .must(QueryBuilders.termQuery(&quot;salesStatus&quot;, 1)) .must(fBuilder); } if (scrollId == null || scrollId.isEmpty()) { if (priceSort == null || priceSort.isEmpty()) { scrollResponse = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setSearchType(SearchType.SCAN).setSize(pageSize).setScroll(TimeValue.timeValueMinutes(5)) .execute() .actionGet(); } else { if ((&quot;desc&quot;).equals(priceSort)) { scrollResponse = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setSearchType(SearchType.SCAN).setSize(pageSize).setScroll(TimeValue.timeValueMinutes(5)) .addSort(&quot;price&quot;, SortOrder.DESC) .execute() .actionGet(); } else { scrollResponse = client.prepareSearch(index).setTypes(type) .setQuery(boolQueryBuilder) .setSearchType(SearchType.SCAN).setSize(pageSize).setScroll(TimeValue.timeValueMinutes(5)) .addSort(&quot;price&quot;, SortOrder.ASC) .execute() .actionGet(); } } //注意:首次搜索并不包含数据，第一次不返回数据，所以先查一次获取scrollId，在进行第二次scroll查询 scrollId = scrollResponse.getScrollId(); } scrollResponse = client.prepareSearchScroll(scrollId) .setScroll(TimeValue.timeValueMinutes(5)) .execute() .actionGet(); //注意:首次搜索并不包含数据，第一次不返回数据，获取总数量 long count = scrollResponse.getHits().getTotalHits(); System.out.println(&quot;scrollResponse.getHits().getHits().length:&quot;+scrollResponse.getHits().hits().length); System.out.println(&quot;scroll count:&quot;+count); //获取本次查询的scrollId scrollIdTemp = scrollResponse.getScrollId(); System.out.println(&quot;--------- searchByScroll scrollID:&quot;+scrollIdTemp); logger.info(&quot;--------- searchByScroll scrollID {}&quot;, scrollIdTemp); // 搜索结果 SearchHit[] searchHits = scrollResponse.getHits().hits(); for (SearchHit searchHit : searchHits) { String source = searchHit.getSource().toString(); logger.info(&quot;--------- searchByScroll source {}&quot;, source); } resultList = responseToList(scrollResponse); } catch (Exception e) { e.printStackTrace(); } return resultList; } /** * 将查询后获得的response转成list * @param response * @return */ public List responseToList(SearchResponse response){ SearchHits hits = response.getHits(); List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;(); System.out.println(&quot;hits.hits().length:&quot;+hits.hits().length); System.out.println(&quot;------------------------&quot;); for (int i = 0; i &lt; hits.hits().length; i++) { Map&lt;String, Object&gt; map = hits.getAt(i).getSource(); Map&lt;String, Object&gt; resultMap = new HashMap&lt;&gt;(16); System.out.println(map.get(&quot;name&quot;)); resultMap.put(&quot;skuId&quot;, map.get(&quot;skuId&quot;)); resultMap.put(&quot;name&quot;, map.get(&quot;name&quot;)); resultMap.put(&quot;image&quot;, map.get(&quot;primaryImage&quot;)); resultMap.put(&quot;price&quot;, map.get(&quot;luckyPrice&quot;)); list.add(resultMap); } System.out.println(&quot;------------------------&quot;); return list; } public TransportClient getClient() { /** * 1:通过 setting对象来指定集群配置信息 * //指定集群名称 * //启动嗅探功能 */ Settings settings = Settings.settingsBuilder() .put(&quot;cluster.name&quot;, clusterName) .put(&quot;client.transport.sniff&quot;, true) .build(); /** * 2：创建客户端 * 通过setting来创建，若不指定则默认链接的集群名为elasticsearch * 链接使用tcp协议即9300 */ TransportClient client = null; try { client = TransportClient.builder().settings(settings).build() .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(HOST), PORT)); System.out.println(&quot;Elasticsearch connect info:&quot; + client.toString()); } catch (UnknownHostException e) { e.printStackTrace(); } /*transportClient = new TransportClient(setting); TransportAddress transportAddress = new InetSocketTransportAddress(&quot;192.168.79.131&quot;, 9300); transportClient.addTransportAddresses(transportAddress);*/ /** * 3：查看集群信息 * 注意我的集群结构是： * 131的elasticsearch.yml中指定为主节点不能存储数据， * 128的elasticsearch.yml中指定不为主节点只能存储数据。 * 所有控制台只打印了192.168.79.128,只能获取数据节点 * */ /*ImmutableList&lt;DiscoveryNode&gt; connectedNodes = client.connectedNodes(); for(DiscoveryNode node : connectedNodes) { System.out.println(node.getHostAddress()); }*/ return client; } /** * 调用 ES 获取 IK 分词后结果 * * @param searchContent * @return */ private List&lt;String&gt; getIkAnalyzeSearchTerms(Client client, String searchContent) { //调用 IK 分词分词 AnalyzeRequestBuilder ikRequest = new AnalyzeRequestBuilder(client, AnalyzeAction.INSTANCE, index, searchContent); //ikRequest.setAnalyzer(&quot;ik&quot;); ikRequest.setTokenizer(&quot;ik&quot;); List&lt;AnalyzeResponse.AnalyzeToken&gt; ikTokenList = ikRequest.execute().actionGet().getTokens(); //循环赋值 List&lt;String&gt; searchTermList = new ArrayList&lt;&gt;(); //ikTokenList.forEach(ikToken -&gt; { searchTermList.add(ikToken.getTerm()); }); for (AnalyzeResponse.AnalyzeToken ikToken: ikTokenList) { System.out.println(ikToken.getTerm()); searchTermList.add(ikToken.getTerm()); } return searchTermList; }} 4. 前端数据为上述的分页的数据可视化，加入前端展示页，可按中文关键词搜索分页等操作。 index.html，以及js操作部分，data.js，由于篇幅问题，下载可移步至github: https://github.com/wonderomg/elasticsearch-visual下载详细内容， 前后端页编写完之后，启动springboot工程，访问http://localhost:8080/该工程端口。 可以看到操作 页面，类似下图： from size和scroll都可通过该页面进行测试。 5. 数据限制问题测试from size时发现一个问题，当数据超过10000条会报错，提示如下： 1234Caused by: QueryPhaseExecutionException[Result window is too large, from + size must be less than or equal to: [10000] but was [10001]. See the scroll api for a more efficient way to request large data sets. This limit can be set by changing the [index.max_result_window] index level parameter.] 原因是es索引被限制了可查前10000条数据，参数是max_result_window，需要我们手动修改这个限制。 (1)查询index.max_result_window 1curl -XGET &quot;10.10.13.234:9200/jd_mall_v2/_settings?preserve_existing=true&quot; (2)更改index.max_result_window 1curl -XPUT &quot;10.10.13.234:9200/jd_mall_v2/_settings?preserve_existing=true&quot; -d '{ &quot;index&quot; : { &quot;max_result_window&quot; : 100000000}}' 设置max_result_window的时候需要先关闭索引，不然会报错Can't update non dynamic settings[[index.preserve_existing]] for open indices的错误，可以通过指令关闭，也可以通过head可视化界面关闭和开启，也可以通过上述步骤1.7和1.8的api关闭和开启。 6. 排序分页的时候我们可以还有排序的场景，java api的话增加代码： 1searchRequestBuilder.addSort(&quot;publish_time&quot;, SortOrder.DESC); 按照某个字段排序的话，hit.getScore()将会失效如果是scroll分页无法进行排序。","link":"/2018/05/17/Elasticsearch%E5%AE%9E%E8%B7%B5api%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/"}],"tags":[{"name":"sprintf","slug":"sprintf","link":"/tags/sprintf/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"ik中文分词","slug":"ik中文分词","link":"/tags/ik%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"},{"name":"alias","slug":"alias","link":"/tags/alias/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"ibatis","slug":"ibatis","link":"/tags/ibatis/"},{"name":"resin","slug":"resin","link":"/tags/resin/"},{"name":"socket","slug":"socket","link":"/tags/socket/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"tcp","slug":"tcp","link":"/tags/tcp/"},{"name":"crontab","slug":"crontab","link":"/tags/crontab/"},{"name":"elemMatch条件操作符","slug":"elemMatch条件操作符","link":"/tags/elemMatch%E6%9D%A1%E4%BB%B6%E6%93%8D%E4%BD%9C%E7%AC%A6/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"分页查询","slug":"分页查询","link":"/tags/%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"集群","slug":"集群","link":"/tags/%E9%9B%86%E7%BE%A4/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"性能优化","slug":"性能优化","link":"/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"跨域","slug":"跨域","link":"/tags/%E8%B7%A8%E5%9F%9F/"},{"name":"随记","slug":"随记","link":"/tags/%E9%9A%8F%E8%AE%B0/"},{"name":"SpingMvc","slug":"SpingMvc","link":"/tags/SpingMvc/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"SpringData","slug":"SpringData","link":"/tags/SpringData/"},{"name":"idea","slug":"idea","link":"/tags/idea/"},{"name":"maven","slug":"maven","link":"/tags/maven/"}],"categories":[{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/categories/elasticsearch/"},{"name":"ibatis","slug":"ibatis","link":"/categories/ibatis/"},{"name":"Socket","slug":"C/Socket","link":"/categories/C/Socket/"},{"name":"tcp","slug":"tcp","link":"/categories/tcp/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"mongodb","slug":"mongodb","link":"/categories/mongodb/"},{"name":"MongoDB","slug":"C/MongoDB","link":"/categories/C/MongoDB/"},{"name":"mysql","slug":"ibatis/mysql","link":"/categories/ibatis/mysql/"},{"name":"mybatis","slug":"mybatis","link":"/categories/mybatis/"},{"name":"zookeeper","slug":"zookeeper","link":"/categories/zookeeper/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Web","slug":"Web","link":"/categories/Web/"},{"name":"resin","slug":"ibatis/mysql/resin","link":"/categories/ibatis/mysql/resin/"},{"name":"随记","slug":"随记","link":"/categories/%E9%9A%8F%E8%AE%B0/"}]}